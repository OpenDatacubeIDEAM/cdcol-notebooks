{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means workflow and notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators import CompressFileSensor\n",
    "from cdcol_utils import other_utils\n",
    "import airflow\n",
    "from airflow.models import DAG\n",
    "from airflow.operators import CDColQueryOperator, CDColFromFileOperator, CDColReduceOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from cdcol_utils import dag_utils, queue_utils, other_utils\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "_params = {'clases': 3, 'minValid': 1, 'normalized': False, 'lat': (2, 3), 'lon': (-74, -73), 'products': [{'name': 'LS8_OLI_LASRC', 'bands': ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'pixel_qa']}], 'time_ranges': [('2020-01-01', '2020-06-30')], 'execID': 'exec_6859', 'elimina_resultados_anteriores': True, 'genera_mosaico': True, 'owner': 'API-REST'}\n",
    "\n",
    "_steps = {\n",
    "    'mascara': {\n",
    "        'algorithm': \"mascara-landsat\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_temporal',\n",
    "            time_range=_params['time_ranges'][0]\n",
    "        ),\n",
    "        'params': {},\n",
    "    },\n",
    "    'reduccion': {\n",
    "        # 'algorithm': \"joiner-reduce\",\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': 'airflow_xlarge',\n",
    "        # 'queue': queue_utils.assign_queue(\n",
    "        #     input_type='multi_temporal_unidad',\n",
    "        #     time_range=_params['time_ranges'][0],\n",
    "        #     unidades=len(_params['products'])\n",
    "        # ),\n",
    "        'params': {},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'medianas': {\n",
    "        'algorithm': \"compuesto-temporal-medianas-wf\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_temporal_unidad',\n",
    "            time_range=_params['time_ranges'][0],\n",
    "            unidades=len(_params['products'])\n",
    "        ),\n",
    "        'params': {\n",
    "            'normalized':_params['normalized'],\n",
    "            'minValid': _params['minValid'],\n",
    "        },\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'mosaico': {\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_area',\n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon']\n",
    "        ),\n",
    "        'params': {},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'k_means': {\n",
    "        'algorithm': \"k-means-wf\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_area',\n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon']\n",
    "        ),\n",
    "        'params': {'clases': _params['clases']},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'owner': _params['owner'],\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'execID': _params['execID'],\n",
    "    'product':_params['products'][0]\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=args[\"execID\"], \n",
    "    default_args=args,\n",
    "    schedule_interval=None,\n",
    "    dagrun_timeout=timedelta(minutes=120)\n",
    ")\n",
    "\n",
    "mascara_0 = dag_utils.queryMapByTile(\n",
    "    lat=_params['lat'], \n",
    "    lon=_params['lon'],\n",
    "    time_ranges=_params['time_ranges'][0],\n",
    "    algorithm=_steps['mascara']['algorithm'],\n",
    "    version=_steps['mascara']['version'],\n",
    "    product=_params['products'][0],\n",
    "    params=_steps['mascara']['params'],\n",
    "    queue=_steps['mascara']['queue'],\n",
    "    dag=dag,\n",
    "    task_id=\"mascara_\" + _params['products'][0]['name']\n",
    ")\n",
    "\n",
    "if len(_params['products']) > 1:\n",
    "    mascara_1 = dag_utils.queryMapByTile(\n",
    "        lat=_params['lat'], \n",
    "        lon=_params['lon'],\n",
    "        time_ranges=_params['time_ranges'][0],\n",
    "        algorithm=_steps['mascara']['algorithm'],\n",
    "        version=_steps['mascara']['version'],\n",
    "        product=_params['products'][1],\n",
    "        params=_steps['mascara']['params'],\n",
    "        queue=_steps['mascara']['queue'], \n",
    "        dag=dag,\n",
    "        task_id=\"mascara_\" + _params['products'][1]['name']\n",
    "    )\n",
    "\n",
    "    reduccion = dag_utils.reduceByTile(\n",
    "        mascara_0 + mascara_1, \n",
    "        algorithm=_steps['reduccion']['algorithm'],\n",
    "        product=_params['products'][0],\n",
    "        version=_steps['reduccion']['version'],\n",
    "        queue=_steps['reduccion']['queue'],\n",
    "        dag=dag,\n",
    "        task_id=\"joined\",\n",
    "        delete_partial_results=_steps['reduccion']['del_prev_result'],\n",
    "        params=_steps['reduccion']['params'],\n",
    "    )\n",
    "else:\n",
    "    reduccion = mascara_0\n",
    "\n",
    "medianas = dag_utils.IdentityMap(\n",
    "    reduccion,\n",
    "    product=_params['products'][0],\n",
    "    algorithm=_steps['medianas']['algorithm'],\n",
    "    version=_steps['medianas']['version'],\n",
    "    task_id=\"medianas\",\n",
    "    queue=_steps['medianas']['queue'],\n",
    "    dag=dag,\n",
    "    delete_partial_results=_steps['medianas']['del_prev_result'],\n",
    "    params=_steps['medianas']['params']\n",
    ")\n",
    "\n",
    "workflow=medianas\n",
    "\n",
    "if  queue_utils.get_tiles(_params['lat'],_params['lon'])>1:\n",
    "    mosaico = dag_utils.OneReduce(\n",
    "        workflow,\n",
    "        task_id=\"mosaic\",\n",
    "        algorithm=_steps['mosaico']['algorithm'],\n",
    "        product=_params['products'][0],\n",
    "        version=_steps['mosaico']['version'], \n",
    "        queue=_steps['mosaico']['queue'],\n",
    "        delete_partial_results=_steps['mosaico']['del_prev_result'],\n",
    "        trigger_rule=TriggerRule.NONE_FAILED,\n",
    "        dag=dag\n",
    "    )\n",
    "    workflow = mosaico\n",
    "\n",
    "kmeans = CDColFromFileOperator(\n",
    "    task_id=\"k_means\",\n",
    "    product=_params['products'][0],\n",
    "    algorithm=_steps['k_means']['algorithm'],\n",
    "    version=_steps['k_means']['version'],\n",
    "    queue=_steps['k_means']['queue'],\n",
    "    dag=dag,\n",
    "    lat=_params['lat'],\n",
    "    lon=_params['lon'],\n",
    "    params=_steps['k_means']['params'],\n",
    "    to_tiff=True\n",
    ")\n",
    "\n",
    "workflow >> kmeans\n",
    "sensor_fin_ejecucion = CompressFileSensor(task_id='sensor_fin_ejecucion',poke_interval=60, soft_fail=True,mode='reschedule', queue='util', dag=dag) \n",
    "comprimir_resultados = PythonOperator(task_id='comprimir_resultados',provide_context=True,python_callable=other_utils.compress_results,queue='util',op_kwargs={'execID': args['execID']},dag=dag) \n",
    "sensor_fin_ejecucion >> comprimir_resultados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(product)\n",
    "print (\"Masking \" + product['name'])\n",
    "nodata=-9999\n",
    "validValues=set()\n",
    "if product['name']==\"LS7_ETM_LEDAPS\" or product['name'] == \"LS5_TM_LEDAPS\":\n",
    "    validValues=[66,68,130,132]\n",
    "elif product['name'] == \"LS8_OLI_LASRC\":\n",
    "    validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "else:\n",
    "    raise Exception(\"Este algoritmo s√≥lo puede enmascarar LS7_ETM_LEDAPS, LS5_TM_LEDAPS o LS8_OLI_LASRC\")\n",
    "\n",
    "cloud_mask = np.isin(xarr0[\"pixel_qa\"].values, validValues)\n",
    "for band in product['bands']:\n",
    "    print(\"entra a enmascarar\")\n",
    "    xarr0[band].values = np.where(np.logical_and(xarr0.data_vars[band] != nodata, cloud_mask), xarr0.data_vars[band], -9999)\n",
    "output = xarr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob, os,sys\n",
    "\n",
    "output=None\n",
    "xarrs=xarrs.values()\n",
    "for _xarr in xarrs:\n",
    "    if (output is None):\n",
    "        output = _xarr\n",
    "    else:\n",
    "        output=output.combine_first(_xarr)\n",
    "\n",
    "#output=xr.auto_combine(list(xarrs))\n",
    "#output=xr.open_mfdataset(\"/source_storage/results/compuesto_de_medianas/compuesto-temporal-medianas-wf_1.0/*.nc\")\n",
    "#output=xr.merge(list(xarrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding=utf8\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "print (\"Compuesto temporal de medianas para \" + product['name'])\n",
    "print(xarr0)\n",
    "nodata=-9999\n",
    "medians = {}\n",
    "time_axis = list(xarr0.coords.keys()).index('time')\n",
    "for band in product['bands']:\n",
    "    print(product['bands'])\n",
    "    if band != 'pixel_qa':\n",
    "        datos = xarr0.data_vars[band].values\n",
    "        allNan = ~np.isnan(datos)\n",
    "\n",
    "        # Comentada por Aurelio (No soporta multi unidad)\n",
    "        #if normalized:\n",
    "        #    m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        if normalized:\n",
    "            m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "            st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "\n",
    "            # Expand m and st according with the data shape\n",
    "            # number of coords\n",
    "            coords_num = len(list(xarr0.coords.keys()))\n",
    "            l = [ x for x in range(coords_num) if x != time_axis]\n",
    "\n",
    "            m_new = m\n",
    "            st_new = st\n",
    "            for axis in l:\n",
    "                # If axis is 0  it is equivalent to x[np.newaxis,:]\n",
    "                # If axis is 1  it is equivalent to x[:,np.newaxis]\n",
    "                # And so on\n",
    "                m_new = np.expand_dims(m_new,axis=axis)\n",
    "                st_new = np.expand_dims(st_new,axis=axis)\n",
    "\n",
    "            print('Time axis',time_axis)\n",
    "            print('New axis',l)\n",
    "            print('m',m.shape)\n",
    "            print('st',st.shape)\n",
    "            print('st_new',st_new.shape)\n",
    "            print('m_new',m_new.shape)\n",
    "            datos=np.true_divide((datos-m_new), st_new)*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        medians[band] = np.nanmedian(datos, time_axis)\n",
    "        medians[band][np.sum(allNan, time_axis) < minValid] = -9999\n",
    "\n",
    "medians[\"ndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"])\n",
    "medians[\"nbr\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])\n",
    "medians[\"nbr2\"]=np.true_divide(medians[\"swir1\"]-medians[\"swir2\"],medians[\"swir1\"]+medians[\"swir2\"])\n",
    "medians[\"ndmi\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])\n",
    "#medians[\"gndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"green\"],medians[\"nir\"]+medians[\"green\"])\n",
    "medians[\"rvi\"]=np.true_divide(medians[\"nir\"],medians[\"red\"])\n",
    "medians[\"nirv\"]=(medians[\"ndvi\"] * medians[\"nir\"])\n",
    "medians[\"osavi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"]+0.16)\n",
    "\n",
    "\n",
    "print('medians_calculated')\n",
    "del datos\n",
    "\n",
    "# > **Asignaci√≥n de coordenadas**\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in xarr0.coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, xarr0.coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=xarr0.coords[x]\n",
    "variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords) for k, v in medians.items()}\n",
    "output=xr.Dataset(variables, attrs={'crs':xarr0.crs})\n",
    "for x in output.coords:\n",
    "    output.coords[x].attrs[\"units\"]=xarr0.coords[x].units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.mlab import PCA\n",
    "from scipy.cluster.vq import kmeans2,vq\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "#Preprocesar:\n",
    "nmed=None\n",
    "nan_mask=None\n",
    "medians1 = xarr0\n",
    "for band in medians1.data_vars.keys():\n",
    "    if band == \"crs\":\n",
    "        continue\n",
    "    b=np.ravel(medians1.data_vars[band].values)\n",
    "    if nan_mask is None:\n",
    "        nan_mask=np.isnan(b)\n",
    "    else:\n",
    "        nan_mask=np.logical_or(nan_mask, np.isnan(medians1.data_vars[band].values.ravel()))\n",
    "    b[np.isnan(b)]=np.nanmedian(b)\n",
    "    if nmed is None:\n",
    "        sp=medians1.data_vars[band].values.shape\n",
    "        nmed=b\n",
    "    else:\n",
    "        nmed=np.vstack((nmed,b))\n",
    "del medians1\n",
    "nodata=-9999\n",
    "#PCA\n",
    "r_PCA=PCA(nmed.T)\n",
    "salida= r_PCA.Y.T.reshape((r_PCA.Y.T.shape[0],)+sp)\n",
    "#Kmeans - 4 clases\n",
    "km_centroids, kmvalues=kmeans2(r_PCA.Y,classes)\n",
    "#Salida:\n",
    "salida[:,nan_mask.reshape(sp)]=np.nan\n",
    "kmv= kmvalues.T.reshape(sp)\n",
    "kmv[nan_mask.reshape(sp)]=nodata\n",
    "coordenadas = []\n",
    "dimensiones =[]\n",
    "xcords = {}\n",
    "for coordenada in xarr0.coords:\n",
    "    if(coordenada != 'time'):\n",
    "        coordenadas.append( ( coordenada, xarr0.coords[coordenada]) )\n",
    "        dimensiones.append(coordenada)\n",
    "        xcords[coordenada] = xarr0.coords[coordenada]\n",
    "valores = {\"kmeans\": xr.DataArray(kmv, dims=dimensiones, coords=coordenadas)}\n",
    "#Genera el dataset (netcdf) con las bandas con el sistema de referencia de coordenadas\n",
    "output = xr.Dataset(valores, attrs={'crs': xarr0.crs})\n",
    "\n",
    "for coordenada in output.coords:\n",
    "    output.coords[coordenada].attrs[\"units\"] = xarr0.coords[coordenada].units"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
