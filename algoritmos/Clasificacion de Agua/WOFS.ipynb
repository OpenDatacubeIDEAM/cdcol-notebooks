{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOFS workflow and notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators import CompressFileSensor\n",
    "from cdcol_utils import other_utils\n",
    "import airflow\n",
    "from airflow.models import DAG\n",
    "from airflow.operators import CDColQueryOperator, CDColFromFileOperator, CDColReduceOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from cdcol_utils import dag_utils, queue_utils, other_utils\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "_params = {'lat': (2, 3), 'lon': (-74, -73), 'products': [{'name': 'LS8_OLI_LASRC', 'bands': ['pixel_qa', 'red', 'blue', 'green', 'nir', 'swir1', 'swir2']}], 'time_ranges': [('2020-01-01', '2020-06-30')], 'execID': 'exec_6857', 'elimina_resultados_anteriores': True, 'genera_mosaico': True, 'owner': 'API-REST'}\n",
    "\n",
    "_steps = {\n",
    "    'wofs': {\n",
    "        'algorithm': \"wofs-wf\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(),\n",
    "    },\n",
    "    'reduccion': {\n",
    "        #'algorithm': \"joiner-reduce\",\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': 'airflow_xlarge',\n",
    "        # 'queue': queue_utils.assign_queue(\n",
    "        #     input_type='multi_temporal_unidad', \n",
    "        #     time_range=_params['time_ranges'][0],\n",
    "        #     unidades=len(_params['products'])\n",
    "        # ),\n",
    "        'params': {},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'serie_tiempo': {\n",
    "        'algorithm': \"wofs-time-series-wf\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_temporal_unidad',\n",
    "            time_range=_params['time_ranges'][0],\n",
    "            unidades=len(_params['products'])\n",
    "        ),\n",
    "        'params': {},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'mosaico': {\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_area', \n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon']\n",
    "        ),\n",
    "        'params': {},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'owner': _params['owner'],\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'execID': _params['execID'],\n",
    "    'product':_params['products'][0]\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=args[\"execID\"], \n",
    "    default_args=args,\n",
    "    schedule_interval=None,\n",
    "    dagrun_timeout=timedelta(minutes=120)\n",
    ")\n",
    "\n",
    "wofs = dag_utils.queryMapByTileByYear(\n",
    "    lat=_params['lat'], \n",
    "    lon=_params['lon'],\n",
    "    time_ranges=_params['time_ranges'][0], \n",
    "    product=_params['products'][0],\n",
    "    algorithm=_steps['wofs']['algorithm'], \n",
    "    version=_steps['wofs']['version'],\n",
    "    queue=_steps['wofs']['queue'],\n",
    "    dag=dag, \n",
    "    task_id=\"wofs\"\n",
    ")\n",
    "\n",
    "reduccion = dag_utils.reduceByTile(\n",
    "    wofs, \n",
    "    algorithm=_steps['reduccion']['algorithm'],\n",
    "    version=_steps['reduccion']['version'],\n",
    "    queue=_steps['reduccion']['queue'], \n",
    "    dag=dag, \n",
    "    task_id=\"joined\",\n",
    "    delete_partial_results=_steps['reduccion']['del_prev_result'],\n",
    "    params=_steps['reduccion']['params'], \n",
    ")\n",
    "\n",
    "serie_tiempo=dag_utils.IdentityMap(\n",
    "    reduccion, \n",
    "    algorithm=_steps['serie_tiempo']['algorithm'],\n",
    "    version=_steps['serie_tiempo']['version'], \n",
    "    task_id=\"wofs_serie_tiempo\",\n",
    "    queue=_steps['serie_tiempo']['queue'], \n",
    "    delete_partial_results=_steps['serie_tiempo']['del_prev_result'],\n",
    "    dag=dag,  \n",
    "    to_tiff= not (_params['genera_mosaico'] and queue_utils.get_tiles(_params['lat'],_params['lon'])>1)\n",
    ")\n",
    "\n",
    "workflow = serie_tiempo\n",
    "if _params['genera_mosaico'] and queue_utils.get_tiles(_params['lat'],_params['lon'])>1:\n",
    "    mosaico = dag_utils.OneReduce(\n",
    "        workflow, \n",
    "        task_id=\"mosaic\", \n",
    "        algorithm=_steps['mosaico']['algorithm'],\n",
    "        version=_steps['mosaico']['version'], \n",
    "        queue=_steps['mosaico']['queue'],\n",
    "        delete_partial_results=_steps['mosaico']['del_prev_result'],\n",
    "        trigger_rule=TriggerRule.NONE_FAILED, \n",
    "        dag=dag, \n",
    "        to_tiff=True\n",
    "    )\n",
    "\n",
    "    workflow = mosaico\n",
    "\n",
    "\n",
    "workflow\n",
    "sensor_fin_ejecucion = CompressFileSensor(task_id='sensor_fin_ejecucion',poke_interval=60, soft_fail=True,mode='reschedule', queue='util', dag=dag) \n",
    "comprimir_resultados = PythonOperator(task_id='comprimir_resultados',provide_context=True,python_callable=other_utils.compress_results,queue='util',op_kwargs={'execID': args['execID']},dag=dag) \n",
    "sensor_fin_ejecucion >> comprimir_resultados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de WOFS adaptado para trabajar en el banco de algoritmos,\n",
    "# las funciones definidas fueron generadas por AMA Team.\n",
    "#\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gc\n",
    "import collections\n",
    "\n",
    "print(\"Ejecutando clasificacion de WOFS\")\n",
    "def wofs_classify(dataset_in, clean_mask=None, no_data=-9999, enforce_float64=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Performs WOfS algorithm on given dataset. If no clean mask is given, the 'cf_mask'\n",
    "      variable must be included in the input dataset, as it will be used to create a\n",
    "      clean mask\n",
    "    Assumption:\n",
    "      - The WOfS algorithm is defined for Landsat 5/Landsat 7\n",
    "    References:\n",
    "      - Mueller, et al. (2015) \"Water observations from space: Mapping surface water from\n",
    "        25 years of Landsat imagery across Australia.\" Remote Sensing of Environment.\n",
    "      - https://github.com/GeoscienceAustralia/eo-tools/blob/stable/eotools/water_classifier.py\n",
    "    -----\n",
    "    Inputs:\n",
    "      dataset_in (xarray.Dataset) - dataset retrieved from the Data Cube; should contain\n",
    "        coordinates: time, latitude, longitude\n",
    "        variables: blue, green, red, nir, swir1, swir2\n",
    "        If user does not provide a clean_mask, dataset_in must also include the cf_mask\n",
    "        variable\n",
    "    Optional Inputs:\n",
    "      clean_mask (nd numpy array with dtype boolean) - true for values user considers clean;\n",
    "        if user does not provide a clean mask, one will be created using cfmask\n",
    "      no_data (int/float) - no data pixel value; default: -9999\n",
    "      enforce_float64 (boolean) - flag to indicate whether or not to enforce float64 calculations;\n",
    "        will use float32 if false\n",
    "    Output:\n",
    "      dataset_out (xarray.DataArray) - wofs water classification results: 0 - not water; 1 - water\n",
    "    \"\"\"\n",
    "\n",
    "    def _band_ratio(a, b):\n",
    "        \"\"\"\n",
    "        Calculates a normalized ratio index\n",
    "        \"\"\"\n",
    "        return np.true_divide((a - b), (a + b))\n",
    "\n",
    "    def _run_regression(band1, band2, band3, band4, band5, band7):\n",
    "        \"\"\"\n",
    "        Regression analysis based on Australia's training data\n",
    "        TODO: Return type\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute normalized ratio indices\n",
    "        ndi_52 = _band_ratio(band5, band2)\n",
    "        ndi_43 = _band_ratio(band4, band3)\n",
    "        ndi_72 = _band_ratio(band7, band2)\n",
    "\n",
    "        # classified = np.ones(shape, dtype='uint8')\n",
    "\n",
    "        classified = np.full(shape, no_data)\n",
    "\n",
    "        # Start with the tree's left branch, finishing nodes as needed\n",
    "\n",
    "        # Left branch\n",
    "        r1 = ndi_52 <= -0.01\n",
    "\n",
    "        r2 = band1 <= 2083.5\n",
    "        classified[r1 & ~r2] = 0  # Node 3\n",
    "\n",
    "        r3 = band7 <= 323.5\n",
    "        _tmp = r1 & r2\n",
    "        _tmp2 = _tmp & r3\n",
    "        _tmp &= ~r3\n",
    "\n",
    "        r4 = ndi_43 <= 0.61\n",
    "        classified[_tmp2 & r4] = 1  # Node 6\n",
    "        classified[_tmp2 & ~r4] = 0  # Node 7\n",
    "\n",
    "        r5 = band1 <= 1400.5\n",
    "        _tmp2 = _tmp & ~r5\n",
    "\n",
    "        r6 = ndi_43 <= -0.01\n",
    "        classified[_tmp2 & r6] = 1  # Node 10\n",
    "        classified[_tmp2 & ~r6] = 0  # Node 11\n",
    "\n",
    "        _tmp &= r5\n",
    "\n",
    "        r7 = ndi_72 <= -0.23\n",
    "        _tmp2 = _tmp & ~r7\n",
    "\n",
    "        r8 = band1 <= 379\n",
    "        classified[_tmp2 & r8] = 1  # Node 14\n",
    "        classified[_tmp2 & ~r8] = 0  # Node 15\n",
    "\n",
    "        _tmp &= r7\n",
    "\n",
    "        r9 = ndi_43 <= 0.22\n",
    "        classified[_tmp & r9] = 1  # Node 17\n",
    "        _tmp &= ~r9\n",
    "\n",
    "        r10 = band1 <= 473\n",
    "        classified[_tmp & r10] = 1  # Node 19\n",
    "        classified[_tmp & ~r10] = 0  # Node 20\n",
    "\n",
    "        # Left branch complete; cleanup\n",
    "        del r2, r3, r4, r5, r6, r7, r8, r9, r10\n",
    "        gc.collect()\n",
    "\n",
    "        # Right branch of regression tree\n",
    "        r1 = ~r1\n",
    "\n",
    "        r11 = ndi_52 <= 0.23\n",
    "        _tmp = r1 & r11\n",
    "\n",
    "        r12 = band1 <= 334.5\n",
    "        _tmp2 = _tmp & ~r12\n",
    "        classified[_tmp2] = 0  # Node 23\n",
    "\n",
    "        _tmp &= r12\n",
    "\n",
    "        r13 = ndi_43 <= 0.54\n",
    "        _tmp2 = _tmp & ~r13\n",
    "        classified[_tmp2] = 0  # Node 25\n",
    "\n",
    "        _tmp &= r13\n",
    "\n",
    "        r14 = ndi_52 <= 0.12\n",
    "        _tmp2 = _tmp & r14\n",
    "        classified[_tmp2] = 1  # Node 27\n",
    "\n",
    "        _tmp &= ~r14\n",
    "\n",
    "        r15 = band3 <= 364.5\n",
    "        _tmp2 = _tmp & r15\n",
    "\n",
    "        r16 = band1 <= 129.5\n",
    "        classified[_tmp2 & r16] = 1  # Node 31\n",
    "        classified[_tmp2 & ~r16] = 0  # Node 32\n",
    "\n",
    "        _tmp &= ~r15\n",
    "\n",
    "        r17 = band1 <= 300.5\n",
    "        _tmp2 = _tmp & ~r17\n",
    "        _tmp &= r17\n",
    "        classified[_tmp] = 1  # Node 33\n",
    "        classified[_tmp2] = 0  # Node 34\n",
    "\n",
    "        _tmp = r1 & ~r11\n",
    "\n",
    "        r18 = ndi_52 <= 0.34\n",
    "        classified[_tmp & ~r18] = 0  # Node 36\n",
    "        _tmp &= r18\n",
    "\n",
    "        r19 = band1 <= 249.5\n",
    "        classified[_tmp & ~r19] = 0  # Node 38\n",
    "        _tmp &= r19\n",
    "\n",
    "        r20 = ndi_43 <= 0.45\n",
    "        classified[_tmp & ~r20] = 0  # Node 40\n",
    "        _tmp &= r20\n",
    "\n",
    "        r21 = band3 <= 364.5\n",
    "        classified[_tmp & ~r21] = 0  # Node 42\n",
    "        _tmp &= r21\n",
    "\n",
    "        r22 = band1 <= 129.5\n",
    "        classified[_tmp & r22] = 1  # Node 44\n",
    "        classified[_tmp & ~r22] = 0  # Node 45\n",
    "\n",
    "        # Completed regression tree\n",
    "\n",
    "        return classified\n",
    "\n",
    "    # Extract dataset bands needed for calculations\n",
    "    blue = dataset_in.blue\n",
    "    green = dataset_in.green\n",
    "    red = dataset_in.red\n",
    "    nir = dataset_in.nir\n",
    "    swir1 = dataset_in.swir1\n",
    "    swir2 = dataset_in.swir2\n",
    "\n",
    "    # Create a clean mask from cfmask if the user does not provide one\n",
    "    if not clean_mask:\n",
    "        cfmask = dataset_in.pixel_qa\n",
    "        clean_mask = create_cfmask_clean_mask(cfmask)\n",
    "\n",
    "    # Enforce float calculations - float64 if user specified, otherwise float32 will do\n",
    "    dtype = blue.values.dtype  # This assumes all dataset bands will have\n",
    "    # the same dtype (should be a reasonable\n",
    "    # assumption)\n",
    "    print (\"El tipo de datos fuente es \" + str(dtype))\n",
    "    # Esto no es necesario si se usa true_divide, el no hacerlo ahorra memoria.\n",
    "    if enforce_float64:\n",
    "        if dtype != 'float64':\n",
    "            blue.values = blue.values.astype('float64')\n",
    "            green.values = green.values.astype('float64')\n",
    "            red.values = red.values.astype('float64')\n",
    "            nir.values = nir.values.astype('float64')\n",
    "            swir1.values = swir1.values.astype('float64')\n",
    "            swir2.values = swir2.values.astype('float64')\n",
    "    else:\n",
    "        if dtype == 'float64':\n",
    "            pass\n",
    "        elif dtype != 'float32':\n",
    "            blue.values = blue.values.astype('float32')\n",
    "            green.values = green.values.astype('float32')\n",
    "            red.values = red.values.astype('float32')\n",
    "            nir.values = nir.values.astype('float32')\n",
    "            swir1.values = swir1.values.astype('float32')\n",
    "            swir2.values = swir2.values.astype('float32')\n",
    "\n",
    "    shape = blue.values.shape\n",
    "    classified = _run_regression(blue.values, green.values, red.values,\n",
    "                                 nir.values, swir1.values, swir2.values)\n",
    "    classified_clean = np.full(classified.shape, no_data, dtype='int16')\n",
    "    classified_clean[clean_mask] = classified[clean_mask]  # Contains data for clear pixels\n",
    "    del classified\n",
    "    # Create xarray of data\n",
    "    time = dataset_in.time\n",
    "    if hasattr(dataset_in, 'latitude'):\n",
    "        latitude = dataset_in.latitude\n",
    "        longitude = dataset_in.longitude\n",
    "\n",
    "        data_array = xr.DataArray(classified_clean.astype(np.int16),\n",
    "                                  coords=[time, latitude, longitude],\n",
    "                                  dims=['time', 'latitude', 'longitude'])\n",
    "\n",
    "        dataset_out = xr.Dataset({'wofs': data_array},\n",
    "                                 coords={'time': time,\n",
    "                                         'latitude': latitude,\n",
    "                                         'longitude': longitude})\n",
    "    else:\n",
    "        y = dataset_in.y\n",
    "        x = dataset_in.x\n",
    "        data_array = xr.DataArray(classified_clean.astype(np.int16),\n",
    "                                  coords=[time, y, x],\n",
    "                                  dims=['time', 'y', 'x'])\n",
    "\n",
    "        dataset_out = xr.Dataset({'wofs': data_array},\n",
    "                                 coords={'time': time,\n",
    "                                         'y': y,\n",
    "                                         'x': x})\n",
    "\n",
    "    return dataset_out\n",
    "\n",
    "\n",
    "def create_cfmask_clean_mask(cfmask):\n",
    "    validValues = set()\n",
    "    if product['name'] == \"LS7_ETM_LEDAPS\" or product['name'] == \"LS5_TM_LEDAPS\":\n",
    "        validValues = [66, 68, 130, 132]\n",
    "    elif product['name'] == \"LS8_OLI_LASRC\":\n",
    "        validValues = [322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "    else:\n",
    "        raise Exception(\"Este algoritmo sÃ³lo puede enmascarar LS7_ETM_LEDAPS, LS5_TM_LEDAPS o LS8_OLI_LASRC\")\n",
    "    clean_mask = np.reshape(np.in1d(cfmask.values.reshape(-1), validValues), cfmask.values.shape)\n",
    "    print(clean_mask)\n",
    "    return clean_mask\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Create a clean mask for clear land/water pixels,\n",
    "      i.e. mask out shadow, snow, cloud, and no data\n",
    "    -----\n",
    "    Input:\n",
    "      cfmask (xarray) - cf_mask from the ledaps products\n",
    "    Output:\n",
    "      clean_mask (boolean numpy array) - clear land/water mask\n",
    "    \"\"\"\n",
    "\n",
    "    #########################\n",
    "    # cfmask values:        #\n",
    "    #   0 - clear           #\n",
    "    #   1 - water           #\n",
    "    #   2 - cloud shadow    #\n",
    "    #   3 - snow            #\n",
    "    #   4 - cloud           #\n",
    "    #   255 - fill          #\n",
    "    #########################\n",
    "\n",
    "\n",
    "def get_spatial_ref(crs):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Get the spatial reference of a given crs\n",
    "    -----\n",
    "    Input:\n",
    "      crs (datacube.model.CRS) - Example: CRS('EPSG:4326')\n",
    "    Output:\n",
    "      ref (str) - spatial reference of given crs\n",
    "    \"\"\"\n",
    "\n",
    "    crs_str = str(crs)\n",
    "    epsg_code = int(crs_str.split(':')[1])\n",
    "    ref = osr.SpatialReference()\n",
    "    ref.ImportFromEPSG(epsg_code)\n",
    "    return str(ref)\n",
    "\n",
    "crs_org=xarr0.crs\n",
    "output = wofs_classify(xarr0)\n",
    "output.attrs[\"crs\"]=crs_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob, os,sys\n",
    "\n",
    "output=None\n",
    "xarrs=xarrs.values()\n",
    "for _xarr in xarrs:\n",
    "    if (output is None):\n",
    "        output = _xarr\n",
    "    else:\n",
    "        output=output.combine_first(_xarr)\n",
    "\n",
    "#output=xr.auto_combine(list(xarrs))\n",
    "#output=xr.open_mfdataset(\"/source_storage/results/compuesto_de_medianas/compuesto-temporal-medianas-wf_1.0/*.nc\")\n",
    "#output=xr.merge(list(xarrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "print(\"Ejecutando calculo de series de tiempo en WOFS\")\n",
    "\n",
    "def perform_timeseries_analysis(dataset_in, no_data=-9999):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "\n",
    "    -----\n",
    "    Input:\n",
    "      dataset_in (xarray.DataSet) - dataset with one variable to perform timeseries on\n",
    "    Output:\n",
    "      dataset_out (xarray.DataSet) - dataset containing\n",
    "        variables: normalized_data, total_data, total_clean\n",
    "    \"\"\"\n",
    "\n",
    "    data_vars = dataset_in.data_vars\n",
    "    key = \"wofs\"\n",
    "\n",
    "    data = data_vars[key]\n",
    "    data.values[np.isnan(data.values)] = no_data\n",
    "    # shape = data.shape[1:]\n",
    "\n",
    "    data_dup = data.copy(deep=True)\n",
    "    data_dup.values = data_dup.values.astype('float')\n",
    "    data_dup.values[data.values == no_data] = 0\n",
    "\n",
    "    processed_data_sum = data_dup.sum('time')\n",
    "    del data_dup\n",
    "    # Masking no data values then converting boolean to int for easy summation\n",
    "    clean_data_raw = np.reshape(np.in1d(data.values.reshape(-1), [no_data], invert=True),\n",
    "                                data.values.shape).astype(int)\n",
    "    # Create xarray of data\n",
    "    time = data.time\n",
    "    print(data.coords)\n",
    "    print(data.dims)\n",
    "    if hasattr(data, \"latitude\"):\n",
    "        latitude = data.latitude\n",
    "        longitude = data.longitude\n",
    "        clean_data = xr.DataArray(clean_data_raw,\n",
    "                                  coords=data.coords,\n",
    "                                  dims=data.dims)\n",
    "    else:\n",
    "        y = data.y\n",
    "        x = data.x\n",
    "        clean_data = xr.DataArray(clean_data_raw,\n",
    "                                  coords=[time, y, x],\n",
    "                                  dims=['time', 'y', 'x'])\n",
    "\n",
    "    clean_data_sum = clean_data.sum('time')\n",
    "\n",
    "    processed_data_normalized = processed_data_sum/clean_data_sum\n",
    "    if hasattr(data, \"latitude\"):\n",
    "        dataset_out = xr.Dataset(collections.OrderedDict(\n",
    "            [('normalized_data', (['latitude', 'longitude'], processed_data_normalized.astype(np.float32))),\n",
    "             ('total_data', (['latitude', 'longitude'], processed_data_sum.astype(np.int16))),\n",
    "             ('total_clean', (['latitude', 'longitude'], clean_data_sum.astype(np.int16)))]),\n",
    "                                 coords={'latitude': latitude,\n",
    "                                         'longitude': longitude})\n",
    "    else:\n",
    "        dataset_out = xr.Dataset(\n",
    "            collections.OrderedDict([('normalized_data', (['y', 'x'], processed_data_normalized.astype(np.float32))),\n",
    "                                     ('total_data', (['y', 'x'], processed_data_sum.astype(np.int16))),\n",
    "                                     ('total_clean', (['y', 'x'], clean_data_sum.astype(np.int16)))]),\n",
    "            coords={'y': y,\n",
    "                    'x': x})\n",
    "    return dataset_out\n",
    "\n",
    "crs_org = xarr0.crs\n",
    "time_series = perform_timeseries_analysis(xarr0)\n",
    "outputs={}\n",
    "outputs[\"time_series\"]=time_series\n",
    "outputs[\"time_series\"].attrs[\"crs\"]=crs_org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
