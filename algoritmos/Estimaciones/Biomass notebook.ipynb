{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomass workflow and notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators import CompressFileSensor\n",
    "from cdcol_utils import other_utils\n",
    "import airflow\n",
    "from airflow.models import DAG\n",
    "from airflow.operators import CDColQueryOperator, CDColFromFileOperator, CDColReduceOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from cdcol_utils import dag_utils, queue_utils, other_utils\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "_params = {'minValid': 1, 'modelos': '/web_storage/downloads/6848', 'normalized': False, 'lat': (2, 3), 'lon': (-74, -73), 'products': [{'name': 'LS7_ETM_LEDAPS', 'bands': ['swir1', 'red', 'nir', 'green', 'blue', 'swir2', 'pixel_qa']}, {'name': 'LS8_OLI_LASRC', 'bands': ['swir1', 'red', 'nir', 'green', 'blue', 'swir2', 'pixel_qa']}], 'time_ranges': [('2020-01-01', '2020-06-30')], 'execID': 'exec_6848', 'elimina_resultados_anteriores': True, 'genera_mosaico': True, 'owner': 'API-REST'}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Biomass Algorithm\n",
    "Templeate created by Crhsitian Segura\n",
    "24-feb-2021\n",
    "\"\"\"\n",
    "'''\n",
    "_params = {\n",
    "        'minValid': 1, \n",
    "        'modelos': '/web_storage/downloads/6758', \n",
    "        'normalized': False, \n",
    "        'lat': (-2, -1),\n",
    "        'lon': (-71, -70),\n",
    "        'products': \n",
    "        [\n",
    "            {\n",
    "                'name': 'LS8_OLI_LASRC', \n",
    "                'bands': ['swir1', 'nir', 'red','swir2', 'pixel_qa']\n",
    "            }\n",
    "        ], \n",
    "        'time_ranges': [('2017-01-01', '2017-12-31')], \n",
    "        'execID': 'biomass_5', \n",
    "        'elimina_resultados_anteriores': True, \n",
    "        'genera_mosaico': True, \n",
    "        'owner': 'cubo_master'\n",
    "        }\n",
    "\n",
    "_params['elimina_resultados_anteriores']=False\n",
    "_params['products'].append(\n",
    "        {\n",
    "            'name':'ALOS2_PALSAR_MOSAIC',\n",
    "            'bands': ['hh','hv']\n",
    "            }\n",
    "        )\n",
    "'''\n",
    "\n",
    "args = {\n",
    "        'owner':  _params['owner'],\n",
    "        'start_date': airflow.utils.dates.days_ago(2),\n",
    "        'execID': _params['execID'],\n",
    "        'product': _params['products'][0]\n",
    "        }\n",
    "\n",
    "dag = DAG(\n",
    "        dag_id=args[\"execID\"],\n",
    "        default_args=args,\n",
    "        schedule_interval=None,\n",
    "        dagrun_timeout=timedelta(minutes=120)\n",
    "        )\n",
    "\n",
    "_steps = {\n",
    "        'mascara': {\n",
    "            'algorithm': \"mascara-landsat\",\n",
    "            'version': '1.0',\n",
    "            'queue': queue_utils.assign_queue(\n",
    "                input_type='multi_temporal',\n",
    "                time_range=_params['time_ranges'][0]\n",
    "                ),\n",
    "            'params': {'bands': _params['products'][0]['bands']}\n",
    "            },\n",
    "        'reduccion': {\n",
    "            #'algorithm': \"joiner-reduce\",\n",
    "            'algorithm': \"joiner\",\n",
    "            'version': '1.0',\n",
    "            #'queue': 'airflow_xlarge',\n",
    "            'queue': queue_utils.assign_queue(\n",
    "                input_type='multi_temporal_unidad',\n",
    "                time_range=_params['time_ranges'][0],\n",
    "                unidades=len(_params['products'])\n",
    "            ),\n",
    "            'params': {'bands': _params['products'][0]['bands']},\n",
    "            'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "            },\n",
    "        'medianas': {\n",
    "            'algorithm': \"compuesto-temporal-medianas-wf\",\n",
    "            'version': '1.0',\n",
    "            #'queue': 'airflow_xlarge',\n",
    "            'queue': queue_utils.assign_queue(\n",
    "                input_type='multi_temporal_unidad',\n",
    "                time_range=_params['time_ranges'][0],\n",
    "                unidades=len(_params['products'])\n",
    "            ),\n",
    "            'params': {\n",
    "                'minValid': _params['minValid'],\n",
    "                'normalized':_params['normalized']\n",
    "                },\n",
    "            'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "            },\n",
    "        'mosaico': {\n",
    "            'algorithm': \"joiner\",\n",
    "            'version': '1.0',\n",
    "            'queue': queue_utils.assign_queue(\n",
    "                input_type='multi_area',\n",
    "                lat=_params['lat'],\n",
    "                lon=_params['lon']\n",
    "                ),\n",
    "            'params': {},\n",
    "            'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "            },\n",
    "        'entrenamiento': {\n",
    "            'algorithm': \"random-forest-training\",\n",
    "            'version': '6.0',\n",
    "            'queue': queue_utils.assign_queue(\n",
    "                input_type='multi_area',\n",
    "                lat=_params['lat'],\n",
    "                lon=_params['lon']\n",
    "                ),\n",
    "            'params': {\n",
    "                'bands': _params['products'][0]['bands'],\n",
    "                'train_data_path': _params['modelos']\n",
    "                },\n",
    "            #'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "            'del_prev_result': False\n",
    "            },\n",
    "        'clasificador': {\n",
    "                'algorithm': \"clasificador-generico-wf\",\n",
    "                'version': '5.0',\n",
    "                'queue': queue_utils.assign_queue(\n",
    "                    input_type='multi_area',\n",
    "                    lat=_params['lat'],\n",
    "                    lon=_params['lon']\n",
    "                    ),\n",
    "                'params': {\n",
    "                    'bands': _params['products'][0]['bands'],\n",
    "                    'modelos': _params['modelos']\n",
    "                    },\n",
    "                'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "                }\n",
    "\n",
    "        }\n",
    "\n",
    "mascara_0 = dag_utils.queryMapByTile(\n",
    "        lat=_params['lat'], \n",
    "        lon=_params['lon'],\n",
    "        time_ranges=_params['time_ranges'][0],\n",
    "        algorithm=_steps['mascara']['algorithm'],\n",
    "        version=_steps['mascara']['version'],\n",
    "        product=_params['products'][0],\n",
    "        params=_steps['mascara']['params'],\n",
    "        queue=_steps['mascara']['queue'],\n",
    "        dag=dag,\n",
    "        task_id=\"mascara_\" + _params['products'][0]['name']\n",
    "        )\n",
    "\n",
    "if len(_params['products']) > 1:\n",
    "    mascara_1 = dag_utils.queryMapByTile(\n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon'],\n",
    "            time_ranges=_params['time_ranges'][0],\n",
    "            algorithm=_steps['mascara']['algorithm'],\n",
    "            version=_steps['mascara']['version'],\n",
    "            product=_params['products'][1],\n",
    "            params=_steps['mascara']['params'],\n",
    "            queue=_steps['mascara']['queue'],\n",
    "            dag=dag,\n",
    "            task_id=\"mascara_\" + _params['products'][1]['name']\n",
    "            )\n",
    "\n",
    "    reduccion = dag_utils.reduceByTile(\n",
    "            mascara_0 + mascara_1,\n",
    "            product=_params['products'][0],\n",
    "            algorithm=_steps['reduccion']['algorithm'],\n",
    "            version=_steps['reduccion']['version'],\n",
    "            queue=_steps['reduccion']['queue'],\n",
    "            dag=dag, task_id=\"joined\",\n",
    "            delete_partial_results=_steps['reduccion']['del_prev_result'],\n",
    "            params=_steps['reduccion']['params'],\n",
    "            )\n",
    "else:\n",
    "    reduccion = mascara_0\n",
    "\n",
    "medianas = dag_utils.IdentityMap(\n",
    "        reduccion,\n",
    "        product=_params['products'][0],\n",
    "        algorithm=_steps['medianas']['algorithm'],\n",
    "        version=_steps['medianas']['version'],\n",
    "        task_id=\"medianas\",\n",
    "        queue=_steps['medianas']['queue'],\n",
    "        dag=dag,\n",
    "        delete_partial_results=_steps['medianas']['del_prev_result'],\n",
    "        params=_steps['medianas']['params']\n",
    "        )\n",
    "\n",
    "workflow=medianas\n",
    "\n",
    "if queue_utils.get_tiles(_params['lat'],_params['lon'])>1:\n",
    "    mosaico = dag_utils.OneReduce(\n",
    "            workflow,\n",
    "            task_id=\"mosaic\",\n",
    "            algorithm=_steps['mosaico']['algorithm'],\n",
    "            version=_steps['mosaico']['version'],\n",
    "            queue=_steps['mosaico']['queue'],\n",
    "            delete_partial_results=_steps['mosaico']['del_prev_result'],\n",
    "            trigger_rule=TriggerRule.NONE_FAILED,\n",
    "            dag=dag\n",
    "            )\n",
    "    workflow=mosaico\n",
    "\n",
    "entrenamiento = dag_utils.IdentityMap(\n",
    "        workflow,\n",
    "        algorithm=_steps['entrenamiento']['algorithm'],\n",
    "        version=_steps['entrenamiento']['version'],\n",
    "        task_id=\"entrenamiento\",\n",
    "        queue=_steps['entrenamiento']['queue'],\n",
    "        dag=dag,\n",
    "        delete_partial_results=_steps['entrenamiento']['del_prev_result'],\n",
    "        params=_steps['entrenamiento']['params']\n",
    "        )\n",
    "\n",
    "clasificador = CDColReduceOperator(\n",
    "        task_id=\"clasificador_generico\",\n",
    "        algorithm=_steps['clasificador']['algorithm'],\n",
    "        version=_steps['clasificador']['version'],\n",
    "        queue=_steps['clasificador']['queue'],\n",
    "        dag=dag,\n",
    "        lat=_params['lat'],\n",
    "        lon=_params['lon'],\n",
    "        params=_steps['clasificador']['params'],\n",
    "        delete_partial_results=_steps['clasificador']['del_prev_result'],\n",
    "        to_tiff=True\n",
    "        )\n",
    "\n",
    "\n",
    "entrenamiento>>clasificador\n",
    "workflow>>clasificador\n",
    "sensor_fin_ejecucion = CompressFileSensor(task_id='sensor_fin_ejecucion',poke_interval=60, soft_fail=True,mode='reschedule', queue='util', dag=dag) \n",
    "comprimir_resultados = PythonOperator(task_id='comprimir_resultados',provide_context=True,python_callable=other_utils.compress_results,queue='util',op_kwargs={'execID': args['execID']},dag=dag) \n",
    "sensor_fin_ejecucion >> comprimir_resultados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(product)\n",
    "print (\"Masking \" + product['name'])\n",
    "nodata=-9999\n",
    "validValues=set()\n",
    "if product['name']==\"LS7_ETM_LEDAPS\" or product['name'] == \"LS5_TM_LEDAPS\":\n",
    "    validValues=[66,68,130,132]\n",
    "elif product['name'] == \"LS8_OLI_LASRC\":\n",
    "    validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "else:\n",
    "    raise Exception(\"Este algoritmo sólo puede enmascarar LS7_ETM_LEDAPS, LS5_TM_LEDAPS o LS8_OLI_LASRC\")\n",
    "\n",
    "cloud_mask = np.isin(xarr0[\"pixel_qa\"].values, validValues)\n",
    "for band in product['bands']:\n",
    "    print(\"entra a enmascarar\")\n",
    "    xarr0[band].values = np.where(np.logical_and(xarr0.data_vars[band] != nodata, cloud_mask), xarr0.data_vars[band], -9999)\n",
    "output = xarr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob, os,sys\n",
    "\n",
    "output=None\n",
    "xarrs=xarrs.values()\n",
    "for _xarr in xarrs:\n",
    "    if (output is None):\n",
    "        output = _xarr\n",
    "    else:\n",
    "        output=output.combine_first(_xarr)\n",
    "\n",
    "#output=xr.auto_combine(list(xarrs))\n",
    "#output=xr.open_mfdataset(\"/source_storage/results/compuesto_de_medianas/compuesto-temporal-medianas-wf_1.0/*.nc\")\n",
    "#output=xr.merge(list(xarrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding=utf8\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "print (\"Compuesto temporal de medianas para \" + product['name'])\n",
    "print(xarr0)\n",
    "nodata=-9999\n",
    "medians = {}\n",
    "time_axis = list(xarr0.coords.keys()).index('time')\n",
    "for band in product['bands']:\n",
    "    if band != 'pixel_qa':\n",
    "        datos = xarr0.data_vars[band].values\n",
    "        allNan = ~np.isnan(datos)\n",
    "\n",
    "        # Comentada por Aurelio (No soporta multi unidad)\n",
    "        #if normalized:\n",
    "        #    m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        if normalized:\n",
    "            m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "            st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "\n",
    "            # Expand m and st according with the data shape\n",
    "            # number of coords\n",
    "            coords_num = len(list(xarr0.coords.keys()))\n",
    "            l = [ x for x in range(coords_num) if x != time_axis]\n",
    "\n",
    "            m_new = m\n",
    "            st_new = st\n",
    "            for axis in l:\n",
    "                # If axis is 0  it is equivalent to x[np.newaxis,:]\n",
    "                # If axis is 1  it is equivalent to x[:,np.newaxis]\n",
    "                # And so on\n",
    "                m_new = np.expand_dims(m_new,axis=axis)\n",
    "                st_new = np.expand_dims(st_new,axis=axis)\n",
    "\n",
    "            print('Time axis',time_axis)\n",
    "            print('New axis',l)\n",
    "            print('m',m.shape)\n",
    "            print('st',st.shape)\n",
    "            print('st_new',st_new.shape)\n",
    "            print('m_new',m_new.shape)\n",
    "            datos=np.true_divide((datos-m_new), st_new)*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        medians[band] = np.nanmedian(datos, time_axis)\n",
    "        medians[band][np.sum(allNan, time_axis) < minValid] = -9999\n",
    "        del datos\n",
    "\n",
    "# > **Asignación de coordenadas**\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in xarr0.coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, xarr0.coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=xarr0.coords[x]\n",
    "variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords) for k, v in medians.items()}\n",
    "output=xr.Dataset(variables, attrs={'crs':xarr0.crs})\n",
    "for x in output.coords:\n",
    "    output.coords[x].attrs[\"units\"]=xarr0.coords[x].units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,posixpath\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gdal\n",
    "import zipfile\n",
    "\n",
    "import datacube\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import json\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from joblib import dump\n",
    "import rasterio.features\n",
    "\n",
    "#parametros:\n",
    "#xarr0: Mosaico del compuesto de medianas\n",
    "#bands: Las bandas a utilizar\n",
    "#train_data_path: Ubicación de los shape files .shp\n",
    "\n",
    "time_ranges = [(\"2017-01-01\", \"2017-12-31\")] #Una lista de tuplas, cada tupla representa un periodo **** mirar de donde viene\n",
    "\n",
    "\n",
    "'''\n",
    "\tCode edited by Crhistian Segura\n",
    " \tDate: 11-Jan-2021\n",
    "\tModif: Created for biomass algorithm\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def geometry_mask(geoms, geobox, all_touched=False, invert=False):\n",
    "    \"\"\"\n",
    "    Create a mask from shapes.\n",
    "    By default, mask is intended for use as a\n",
    "    numpy mask, where pixels that overlap shapes are False.\n",
    "    :param list[Geometry] geoms: geometries to be rasterized\n",
    "    :param datacube.utils.GeoBox geobox:\n",
    "    :param bool all_touched: If True, all pixels touched by geometries will be burned in. If\n",
    "                             false, only pixels whose center is within the polygon or that\n",
    "                             are selected by Bresenham's line algorithm will be burned in.\n",
    "    :param bool invert: If True, mask will be True for pixels that overlap shapes.\n",
    "    \"\"\"\n",
    "    return rasterio.features.geometry_mask([geom.to_crs(geobox.crs) for geom in geoms],\n",
    "            out_shape=geobox.shape,\n",
    "            transform=geobox.affine,\n",
    "            all_touched=all_touched,\n",
    "            invert=invert)\n",
    "\n",
    "# Load csv files\n",
    "train_zip_file_name  = train_data_path\n",
    "\n",
    "Conglomerados = pd.read_csv(train_data_path+'/conglomerados.csv',delimiter=',')\n",
    "#data_set_PPM = pd.read_csv('data_set.csv',delimiter=',')\n",
    "\n",
    "REFdata = Conglomerados\n",
    "REFdata['random1'] = np.random.randint(1,11,Conglomerados.shape[0])\n",
    "\n",
    "\n",
    "## local variables\n",
    "# Number of folds (k) to run (Kmax should be equal tot he number of k-folds of the training dataset, but you can run less kfolds for debugging purposes)\n",
    "Kmax = 10\n",
    "\n",
    "# Point at the attributes names of your shapefile\n",
    "DV = 'Cha_HD' #Dependent variable name (column with you variable to predict)\n",
    "#Variable (column) with the folds (if k=10, then the values should be from 1 to 10). Two options\n",
    "#- 'kfold'-> kfold will be randomly sampled \n",
    "#- 'k10' -> kfold will use a prepared stratified random sampling (shapefile attribute) \n",
    "\n",
    "kfold = 'k10'\n",
    "\n",
    "#Variable that count the number of subplot within the cluster\n",
    "countCluster = 'Count'  \n",
    "Nplots = 5; #Number of subplots\n",
    "\n",
    "#Set a maximum value for your dependent variable\n",
    "maxValueplots = 500\n",
    "\n",
    "train_kfold = REFdata[REFdata['1ha']==1]\n",
    "train_kfold = train_kfold[train_kfold[countCluster]>= Nplots]\n",
    "train_kfold = train_kfold[train_kfold[DV] < maxValueplots]\n",
    "print(f'kfold dataset: {train_kfold.shape}');\n",
    "\n",
    "\n",
    "## Generacion de datos de entrenamiento a partir de los conglomerados\n",
    "salidas=[]\n",
    "training_labels_all=[]\n",
    "training_samples_all=np.array([], dtype=np.int64).reshape(0,7)\n",
    "for i in range(len(train_kfold.Cha_HD)):\n",
    "    print(f'Running conglom {i+1}')\n",
    "    try:\n",
    "        a = json.loads(train_kfold.iloc[i]['.geo'])\n",
    "\n",
    "        geom = geometry.Geometry(a,crs=CRS('EPSG:4326'))\n",
    "\n",
    "        dc = datacube.Datacube(app=\"Cana\")\n",
    "        \"\"\"\n",
    "        ALOS = dc.load(\n",
    "            product='ALOS2_PALSAR_MOSAIC',\n",
    "            geopolygon=geom,\n",
    "        )\n",
    "\n",
    "        ALOS=ALOS.isel(time=0)\n",
    "        ALOS\n",
    "        \"\"\"\n",
    "        #for i in range(30):\n",
    "        xarr_0 = dc.load(\n",
    "            product='LS7_ETM_LEDAPS_MOSAIC',\n",
    "            time=(\"2016-01-01\", \"2016-12-31\"),\n",
    "            geopolygon=geom,\n",
    "        )\n",
    "\n",
    "        \n",
    "        mask = geometry_mask([geom], xarr_0.geobox, invert=True)\n",
    "        \n",
    "        xarr_0 = xarr_0.where(mask)\n",
    "        xarr_0=xarr_0.isel(time=0)\n",
    "\n",
    "\n",
    "        '''\n",
    "        # mascara de nubes\n",
    "        import numpy as np\n",
    "\n",
    "        nodata=-9999\n",
    "\t    \n",
    "        validValues=set()\n",
    "        if product['name']==\"LS7_ETM_LEDAPS\" or product['name'] == \"LS5_TM_LEDAPS\":\n",
    "            validValues=[66,68,130,132]\n",
    "        elif product['name'] == \"LS8_OLI_LASRC\":\n",
    "            validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "        else:\n",
    "            raise Exception(\"Este algoritmo solo puede enmascarar LS7_ETM_LEDAPS, LS5_TM_LEDAPS o LS8_OLI_LASRC\")\n",
    "\n",
    "        cloud_mask = np.isin(xarr_0[\"pixel_qa\"].values, validValues)\n",
    "        for band in product['bands']:\n",
    "            xarr_0[band].values = np.where(np.logical_and(xarr_0.data_vars[band] != nodata, cloud_mask), xarr_0.data_vars[band], np.nan)\n",
    "        xarr_mask = xarr_0\n",
    "\n",
    "        # comppuesto de medianas\n",
    "        normalized = True\n",
    "        minValid = 1\n",
    "\n",
    "        medians={} \n",
    "        for band in product['bands']:\n",
    "             if band != 'pixel_qa':\n",
    "                datos=xarr_mask[band].values\n",
    "                allNan=~np.isnan(datos) #Una mascara que indica qué datos son o no nan. \n",
    "                if normalized: #Normalizar, si es necesario.\n",
    "                    #Para cada momento en el tiempo obtener el promedio y la desviación estándar de los valores de reflectancia\n",
    "                    m=np.nanmean(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "                    st=np.nanstd(datos.reshape((datos.shape[0],-1)), axis=1)\n",
    "                    # usar ((x-x̄)/st) para llevar la distribución a media 0 y desviación estándar 1,\n",
    "                    # y luego hacer un cambio de espacio para la nueva desviación y media. \n",
    "                    datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "                #Calcular la mediana en la dimensión de tiempo \n",
    "                medians[band]=np.nanmedian(datos,0) \n",
    "                #Eliminar los valores que no cumplen con el número mínimo de pixeles válidos dado. \n",
    "                medians[band][np.sum(allNan,0)<minValid]=np.nan\n",
    "\n",
    "        del datos\n",
    "        '''\n",
    "        medians={} \n",
    "        # normalizar bandas\n",
    "        medians[\"red\"]   = xarr_0[\"red\"]  /10000\n",
    "        medians[\"nir\"]   = xarr_0[\"nir\"]  /10000\n",
    "        medians[\"swir1\"] = xarr_0[\"swir1\"]/10000\n",
    "        medians[\"swir2\"] = xarr_0[\"swir2\"]/10000\n",
    "       \n",
    "\n",
    "        # Calculo de indices\n",
    "        medians[\"ndvi\"]=(medians[\"nir\"]-medians[\"red\"])/(medians[\"nir\"]+medians[\"red\"])\n",
    "        medians[\"nbr\"] =(medians[\"nir\"]-medians[\"swir2\"])/(medians[\"nir\"]+medians[\"swir2\"])\n",
    "        #medians[\"nbr2\"]=(medians[\"swir1\"]-medians[\"swir2\"])/(medians[\"swir1\"]+medians[\"swir2\"])\n",
    "        #medians[\"ndmi\"]=(medians[\"nir\"]-medians[\"swir1\"])/(medians[\"nir\"]+medians[\"swir1\"])\n",
    "        medians[\"savi\"]=((medians[\"nir\"]-medians[\"red\"])/(medians[\"nir\"]+medians[\"red\"] + (0.5)))*(1.5)\n",
    "\n",
    "        # **Asignacion de coordenadas**\n",
    "        ncoords=[]\n",
    "        xdims =[]\n",
    "        xcords={}\n",
    "        for x in xarr_0.coords:\n",
    "            if(x!='time'):\n",
    "                ncoords.append( ( x, xarr_0.coords[x]) )\n",
    "                xdims.append(x)\n",
    "                xcords[x]=xarr_0.coords[x]\n",
    "        variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords) for k, v in medians.items()}\n",
    "        output0=xr.Dataset(variables, attrs={'crs':xarr_0.crs})\n",
    "        for x in output0.coords:\n",
    "            output0.coords[x].attrs[\"units\"]=xarr_0.coords[x].units\n",
    "        output0\n",
    "        \"\"\"\n",
    "        # Calbration factor\n",
    "        # Convertir a .astype(np.int32) para evitar truncamiento\n",
    "        ALOS32 = ALOS.astype(np.int32)\n",
    "        # https://www.eorc.jaxa.jp/ALOS-2/en/calval/calval_index.htm\n",
    "        ALOS_cal = (np.log10(ALOS32*ALOS32)*10)-83\n",
    "        ALOS_2 = 10**(0.1*ALOS_cal)\n",
    "        rfid = (ALOS_2.hh-ALOS_2.hv)/(ALOS_2.hh+ALOS_2.hv)\n",
    "        cpR = ALOS_2.hv/ALOS_2.hh\n",
    "\n",
    "        ALOS_2\n",
    "        \n",
    "\n",
    "        #output0['hh'] = ALOS_2.hh\n",
    "        #output0['hv'] = ALOS_2.hv\n",
    "        #output0['rfid']=rfid\n",
    "        #output0['cpR'] =cpR\n",
    "        \"\"\"\n",
    "        \n",
    "        output0=output0.where(mask)\n",
    "        \n",
    "        labeled_pixels=mask*1\n",
    "                \n",
    "        is_train = np.nonzero(labeled_pixels)\n",
    "        training_labels = labeled_pixels[is_train]\n",
    "        \n",
    "\n",
    "        bands_data=[]\n",
    "        for band in output0.data_vars.keys():\n",
    "            # pixel_qa is removed from xarr0 by Compuesto Temporal de Medianas\n",
    "            if band != 'pixel_qa':\n",
    "                bands_data.append(output0[band])\n",
    "        bands_data = np.dstack(bands_data)\n",
    "\n",
    "        # limpiar bands_data[is_train] de nan\n",
    "        \n",
    "        training_samples_all = np.concatenate((training_samples_all, bands_data[is_train]))\n",
    "        training_labels_all= np.concatenate((training_labels_all,training_labels*train_kfold.iloc[i].Cha_HD))\n",
    "        \n",
    "        salidas.append(output0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Failed: '+ str(e))\n",
    "        \n",
    "\n",
    "print(f'training_samples: {training_samples_all.shape}')\n",
    "print(f'training_labels: {training_labels_all.shape}')\n",
    "\n",
    "training_pd_samples=pd.DataFrame(training_samples_all)\n",
    "training_pd_labels=pd.DataFrame(training_labels_all, columns={DV})\n",
    "\n",
    "nullData=training_pd_samples[0].isnull()\n",
    "\n",
    "all_data=pd.concat((training_pd_samples[~nullData],training_pd_labels[~nullData]),axis=1)\n",
    "\n",
    "all_data_shuf=all_data.sample(frac=1).reset_index(drop=True)\n",
    "all_data_shuf=all_data_shuf.dropna()\n",
    "# Select data for clasification\n",
    "X=all_data_shuf.loc[:,all_data_shuf.columns!=DV]\n",
    "y=all_data_shuf[DV].to_numpy()\n",
    "\n",
    "print(f'X shape:{X.shape}')\n",
    "print(f'y shape:{y.shape}')\n",
    "\n",
    "\n",
    "# Estimador con los mismos parametros de gee\n",
    "clf = RandomForestRegressor(n_estimators = 500,\n",
    "                            max_features = 'sqrt',\n",
    "                            min_samples_leaf = 1,\n",
    "                            #bootstrap = True,\n",
    "                            #max_samples=0.5,\n",
    "                            max_leaf_nodes = None,\n",
    "                            random_state = 123)\n",
    "\n",
    "# K-Fold para la generacion de 10 modelos de RF ( entrenamiento )\n",
    "cv_results = cross_validate(clf, X, y, cv = 10,\n",
    "                            scoring = ('r2'),\n",
    "                            return_estimator=True,\n",
    "                            return_train_score=True)\n",
    "\n",
    "# imprim los r2 de los 10 modelos con la data de entrenamiento\n",
    "for i in range(10):\n",
    "    ya = cv_results['estimator'][i].predict(X)\n",
    "    print(f'modelo {i} r2: {r2_score(y, ya):0.4f}')\n",
    "\n",
    "# Export models to folder\n",
    "import os\n",
    "\n",
    "if not os.path.exists(posixpath.join(folder,'models')):\n",
    "    os.makedirs(posixpath.join(folder,'models'))\n",
    "\n",
    "from joblib import dump, load\n",
    "for i in range(10):\n",
    "    dump(cv_results['estimator'][i], posixpath.join(folder,'models/modelo_'+str(i)+'.joblib'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# The trainning data must be in a zip folder.\n",
    "train_zip_file_name  = [file_name for file_name in os.listdir(train_data_path) if file_name.endswith('.zip')][0]\n",
    "train_zip_file_path = os.path.join(train_data_path,train_zip_file_name)\n",
    "train_folder_path = train_zip_file_path.replace('.zip','')\n",
    "\n",
    "print('train_zip_file_path',train_zip_file_path)\n",
    "print('train_folder_path',train_folder_path)\n",
    "\n",
    "zip_file = zipfile.ZipFile(train_zip_file_path)\n",
    "zip_file.extractall(train_data_path)\n",
    "zip_file.close()\n",
    "\n",
    "#files = [f for f in os.listdir(train_data_path) if f.endswith('.shp')]\n",
    "files = [f for f in os.listdir(train_folder_path) if f.endswith('.shp')]\n",
    "classes = [f.split('.')[0] for f in files]\n",
    "print(classes)\n",
    "#shapefiles = [os.path.join(train_data_path, f) for f in files if f.endswith('.shp')]\n",
    "shapefiles = [os.path.join(train_folder_path, f) for f in files if f.endswith('.shp')]\n",
    "\n",
    "print('shapefile',shapefiles)\n",
    "\n",
    "labeled_pixels = rasterizar_entrenamiento(shapefiles, rows, cols, geo_transform, proj)\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier(n_jobs=-1, n_estimators=500)\n",
    "\n",
    "print('trainning samples',X_train)\n",
    "print('trainning labels',y_train)\n",
    "#classifier = RandomForestClassifier(n_jobs=-1, n_estimators=50, verbose=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Calculo de y_pred\n",
    "print('Estimar y con datos de entrada')\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculo de matrix de confusion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, precision_score\n",
    "\n",
    "mconf = confusion_matrix(y_test,y_pred)\n",
    "# Calculo de kappa score\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "# Calculo de precision score\n",
    "prec = precision_score(y_test, y_pred,average = 'weighted')\n",
    "\n",
    "# Save metrics to file\n",
    "with open(posixpath.join(folder+'metrics.txt'),'w') as file_metrics:\n",
    "    print(f'matriz de confusion: {mconf}')\n",
    "    print(f'kappa score: {kappa}')\n",
    "    print(f'precision score (weighted): {prec}')\n",
    "    file_metrics.write('matriz de confusion: \\n'+str(mconf))\n",
    "    file_metrics.write('\\nkappa score: '+str(kappa))\n",
    "    file_metrics.write('\\nprecision score (weighted): '+str(prec))\n",
    "\n",
    "# write shapefiles list\n",
    "file = open(folder+\"shapefiles_list.txt\", \"w\")\n",
    "file.write(\"shapefiles list = \" + \"\\n\".join(shapefiles))\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "outputxcom=posixpath.join(folder,'modelo_random_forest.pkl')\n",
    "with open(outputxcom, 'wb') as fid:\n",
    "    print('output',classifier)\n",
    "    joblib.dump(classifier, fid)\n",
    "\n",
    "print(classifier)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "import warnings\n",
    "\n",
    "import os, posixpath\n",
    "\n",
    "# Preprocesar:\n",
    "xarrs=list(xarrs.values())\n",
    "print(type(xarrs))\n",
    "output0 = xarrs[0]\n",
    "print(type(output0))\n",
    "print(f'len xarrs: {len(xarrs)}')\n",
    "print(f'xarrs: {xarrs}')\n",
    "\n",
    "\n",
    "print(\"medianas\")\n",
    "print(f'bandas: {list(output0.data_vars.keys())}')\n",
    "print(output0)\n",
    "bands_data=[]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  Revisar las medianas parecen tener inconvenientes, el dato que retorna tiene mas de un slide de tiempo\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Escalar bandas\n",
    "\n",
    "output0[\"red\"]=output0[\"red\"]/10000\n",
    "output0[\"nir\"]=output0[\"nir\"]/10000\n",
    "output0[\"swir1\"]=output0[\"swir1\"]/10000\n",
    "output0[\"swir2\"]=output0[\"swir2\"]/10000\n",
    "\n",
    "# Agregar Indices\n",
    "\n",
    "output0[\"ndvi\"]=(output0[\"nir\"]-output0[\"red\"])/(output0[\"nir\"]+output0[\"red\"])\n",
    "output0[\"nbr\"] =(output0[\"nir\"]-output0[\"swir2\"])/(output0[\"nir\"]+output0[\"swir2\"])\n",
    "output0[\"savi\"]=((output0[\"nir\"]-output0[\"red\"])/(output0[\"nir\"]+output0[\"red\"] + (0.5)))*(1+0.5)\n",
    "\n",
    "print(f'Data + indices: {output0}')\n",
    "\n",
    "\n",
    "for band in output0.data_vars.keys():\n",
    "    # pixel_qa is removed from xarr0 by Compuesto Temporal de Medianas\n",
    "    if band != 'pixel_qa':\n",
    "        bands_data.append(output0[band])\n",
    "    print(f'band processed: {band}')\n",
    "\n",
    "bands_data = np.dstack(bands_data)\n",
    "\n",
    "\n",
    "rows, cols, n_bands = bands_data.shape\n",
    "\n",
    "n_samples = rows*cols\n",
    "flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "\n",
    "where_are_NaNs = np.isnan(flat_pixels)\n",
    "print(f'NaNs in data : {where_are_NaNs.sum()}')\n",
    "flat_pixels[where_are_NaNs] = -9999\n",
    "\n",
    "where_are_Infs = np.isinf(flat_pixels)\n",
    "print(f'Infs in data : {where_are_Infs .sum()}')\n",
    "flat_pixels[where_are_Infs] = -9999\n",
    "\n",
    "\n",
    "print(f'Flat_pixels shape: {flat_pixels.shape}')\n",
    "print(f'Flat_pixels: {flat_pixels}')\n",
    "\n",
    "# Cargar modelos\n",
    "\n",
    "# Clasificacion de los 10 mapas\n",
    "y_maps=[]\n",
    "\n",
    "print(os.path.dirname(folder))\n",
    "print('Cargar los modelos')\n",
    "for i in range(10):\n",
    "    clf_load = load(posixpath.join(os.path.dirname(os.path.dirname(folder)),'random-forest-training_6.0/models/modelo_'+str(i)+'.joblib'))\n",
    "    print(f'Cargado modelo {i}')\n",
    "    y_maps.append(clf_load.predict(flat_pixels))\n",
    "\n",
    "# Generar 10 imagenes\n",
    "kClasificaciones=[]\n",
    "for i in range(10):\n",
    "    y_maps[i][where_are_NaNs[:,0]]=np.nan\n",
    "    classification = y_maps[i].reshape((rows, cols))\n",
    "    kClasificaciones.append(classification)\n",
    "\n",
    "\n",
    "# union de mapas con la mediana\n",
    "\n",
    "result = np.apply_along_axis(np.mean,0,kClasificaciones)\n",
    "\n",
    "print('result classification',result)\n",
    "\n",
    "\n",
    "coordenadas = []\n",
    "dimensiones = []\n",
    "xcords = {}\n",
    "for coordenada in xarrs[0].coords:\n",
    "    if (coordenada != 'time'):\n",
    "        coordenadas.append((coordenada, xarrs[0].coords[coordenada]))\n",
    "        dimensiones.append(coordenada)\n",
    "        xcords[coordenada] = xarrs[0].coords[coordenada]\n",
    "\n",
    "valores = {\"classified\": xr.DataArray(result, dims=dimensiones, coords=coordenadas)}\n",
    "#array = xr.DataArray(result, dims=dimensiones, coords=coordenadas)\n",
    "#array.astype('float32')\n",
    "#valores = {\"classified\": array}\n",
    "print('creacion mapa biomasa')\n",
    "\n",
    "\n",
    "biomasa = xr.Dataset(valores, attrs={'crs': xarrs[0].crs})\n",
    "for coordenada in output0.coords:\n",
    "    biomasa.coords[coordenada].attrs[\"units\"] = xarrs[0].coords[coordenada].units\n",
    "print('creacion mapa carbono')\n",
    "\n",
    "carbono = biomasa.copy()*0.47\n",
    "\n",
    "print('preparacion salidas')\n",
    "outputs = {'biomasa': biomasa,\n",
    "'carbono': carbono }\n",
    "print(f'outputs:{outputs}')\n",
    "\n",
    "\n",
    "classified = biomasa.classified\n",
    "classified.values = classified.values.astype('float32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
