{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de cicatrices de incendios (RF) workflow and notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators import CompressFileSensor\n",
    "from cdcol_utils import other_utils\n",
    "import airflow\n",
    "from airflow.models import DAG\n",
    "from airflow.operators import CDColQueryOperator, CDColFromFileOperator, CDColReduceOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from cdcol_utils import dag_utils, queue_utils, other_utils\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "_params = {'minValid': 1, 'modelos': '/web_storage/downloads/6865', 'normalized': False, 'lat': (2, 3), 'lon': (-74, -73), 'products': [{'name': 'LS7_ETM_LEDAPS', 'bands': ['swir1', 'nir', 'red', 'green', 'blue', 'swir2', 'pixel_qa']}, {'name': 'LS8_OLI_LASRC', 'bands': ['swir1', 'nir', 'red', 'green', 'blue', 'swir2', 'pixel_qa']}], 'time_ranges': [('2020-01-01', '2020-06-30')], 'execID': 'exec_6865', 'elimina_resultados_anteriores': True, 'genera_mosaico': True, 'owner': 'API-REST'}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Modified by: Crhistian Segura\n",
    "Date: 04-12-2020\n",
    "\n",
    "_params = {'minValid': 1, 'modelos': '/web_storage/downloads/6418', 'normalized': True,\n",
    "\t   'lat': (5, 6), 'lon': (-74, -73), 'products': [{'name': 'LS7_ETM_LEDAPS', \n",
    "\t   'bands': ['swir2', 'pixel_qa', 'swir1', 'green', 'red', 'blue', 'nir']}, \n",
    "\t   {'name': 'DEM_Mosaico', 'bands': ['dem']}],\n",
    "\t   'time_ranges': [('2015-08-01', '2015-09-30')], 'execID': 'a_cicatriz_5',\n",
    "\t   'elimina_resultados_anteriores': True, 'genera_mosaico': True, 'owner':\n",
    "    \t   'API-REST'}\n",
    "\"\"\"\n",
    "_params['elimina_resultados_anteriores']=False\n",
    "\n",
    "args = {\n",
    "    'owner':  _params['owner'],\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'execID': _params['execID'],\n",
    "    'product': _params['products'][0]\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=args[\"execID\"],\n",
    "    default_args=args,\n",
    "    schedule_interval=None,\n",
    "    dagrun_timeout=timedelta(minutes=120)\n",
    ")\n",
    "\n",
    "_steps = {\n",
    "    'mascara': {\n",
    "        'algorithm': \"mascara-landsat\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_temporal',\n",
    "            time_range=_params['time_ranges'][0]\n",
    "        ),\n",
    "        'params': {'bands': _params['products'][0]['bands']}\n",
    "    },\n",
    "    'reduccion': {\n",
    "        #'algorithm': \"joiner-reduce\",\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': 'airflow_xlarge',\n",
    "        # 'queue': queue_utils.assign_queue(\n",
    "        #     input_type='multi_temporal_unidad',\n",
    "        #     time_range=_params['time_ranges'][0],\n",
    "        #     unidades=len(_params['products'])\n",
    "        # ),\n",
    "        'params': {'bands': _params['products'][0]['bands']},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'medianas': {\n",
    "        'algorithm': \"compuesto-temporal-medianas-indices-wf\",\n",
    "        'version': '4.0',\n",
    "        'queue': 'airflow_xlarge',\n",
    "        #'queue': queue_utils.assign_queue(\n",
    "        #    input_type='multi_temporal_unidad',\n",
    "        #    time_range=_params['time_ranges'][0],\n",
    "        #    unidades=len(_params['products'])\n",
    "        #),\n",
    "        'params': {\n",
    "            'minValid': _params['minValid'],\n",
    "            'normalized':_params['normalized']\n",
    "        },\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'mosaico': {\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_area',\n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon']\n",
    "        ),\n",
    "        'params': {},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'entrenamiento': {\n",
    "        'algorithm': \"random-forest-training\",\n",
    "        'version': '5.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_area',\n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon']\n",
    "        ),\n",
    "        'params': {\n",
    "            'bands': _params['products'][0]['bands'],\n",
    "            'train_data_path': _params['modelos']\n",
    "        },\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "        #'del_prev_result': False\n",
    "    },\n",
    "    'clasificador': {\n",
    "        'algorithm': \"clasificador-generico-wf\",\n",
    "        'version': '4.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_area',\n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon']\n",
    "        ),\n",
    "        'params': {\n",
    "            'bands': _params['products'][0]['bands'],\n",
    "            'modelos': _params['modelos']\n",
    "        },\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "mascara_0 = dag_utils.queryMapByTile(\n",
    "    lat=_params['lat'], \n",
    "    lon=_params['lon'],\n",
    "    time_ranges=_params['time_ranges'][0],\n",
    "    algorithm=_steps['mascara']['algorithm'],\n",
    "    version=_steps['mascara']['version'],\n",
    "    product=_params['products'][0],\n",
    "    params=_steps['mascara']['params'],\n",
    "    queue=_steps['mascara']['queue'],\n",
    "    dag=dag,\n",
    "    task_id=\"mascara_\" + _params['products'][0]['name']\n",
    ")\n",
    "\n",
    "if len(_params['products']) > 1:\n",
    "    mascara_1 = dag_utils.queryMapByTile(\n",
    "        lat=_params['lat'],\n",
    "        lon=_params['lon'],\n",
    "        time_ranges=('2013-01-01',' 2013-12-31'),\n",
    "        algorithm=_steps['mascara']['algorithm'],\n",
    "        version='2.0',\n",
    "        product=_params['products'][1],\n",
    "        params=_steps['mascara']['params'],\n",
    "        queue=_steps['mascara']['queue'],\n",
    "        dag=dag,\n",
    "        task_id=\"mascara_\" + _params['products'][1]['name']\n",
    "    )\n",
    "\n",
    "    reduccion = dag_utils.reduceByTile(\n",
    "        mascara_0 + mascara_1,\n",
    "        product=_params['products'][0],\n",
    "        algorithm=_steps['reduccion']['algorithm'],\n",
    "        version=_steps['reduccion']['version'],\n",
    "        queue=_steps['reduccion']['queue'],\n",
    "        dag=dag, task_id=\"joined\",\n",
    "        delete_partial_results=_steps['reduccion']['del_prev_result'],\n",
    "        params=_steps['reduccion']['params'],\n",
    "    )\n",
    "else:\n",
    "    reduccion = mascara_0\n",
    "\n",
    "medianas = dag_utils.IdentityMap(\n",
    "    reduccion,\n",
    "    product=_params['products'][0],\n",
    "    algorithm=_steps['medianas']['algorithm'],\n",
    "    version=_steps['medianas']['version'],\n",
    "    task_id=\"medianas\",\n",
    "    queue=_steps['medianas']['queue'],\n",
    "    dag=dag,\n",
    "    delete_partial_results=_steps['medianas']['del_prev_result'],\n",
    "    params=_steps['medianas']['params']\n",
    ")\n",
    "\n",
    "workflow=medianas\n",
    "\n",
    "if queue_utils.get_tiles(_params['lat'],_params['lon'])>1:\n",
    "    mosaico = dag_utils.OneReduce(\n",
    "        workflow,\n",
    "        task_id=\"mosaic\",\n",
    "        algorithm=_steps['mosaico']['algorithm'],\n",
    "        version=_steps['mosaico']['version'],\n",
    "        queue=_steps['mosaico']['queue'],\n",
    "        delete_partial_results=_steps['mosaico']['del_prev_result'],\n",
    "        trigger_rule=TriggerRule.NONE_FAILED,\n",
    "        dag=dag\n",
    "    )\n",
    "\n",
    "    workflow=mosaico\n",
    "\n",
    "entrenamiento = dag_utils.IdentityMap(\n",
    "    workflow,\n",
    "    algorithm=_steps['entrenamiento']['algorithm'],\n",
    "    version=_steps['entrenamiento']['version'],\n",
    "    task_id=\"entrenamiento\",\n",
    "    queue=_steps['entrenamiento']['queue'],\n",
    "    dag=dag,\n",
    "    delete_partial_results=_steps['entrenamiento']['del_prev_result'],\n",
    "    params=_steps['entrenamiento']['params']\n",
    ")\n",
    "\n",
    "clasificador = CDColReduceOperator(\n",
    "    task_id=\"clasificador_generico\",\n",
    "    algorithm=_steps['clasificador']['algorithm'],\n",
    "    version=_steps['clasificador']['version'],\n",
    "    queue=_steps['clasificador']['queue'],\n",
    "    dag=dag,\n",
    "    lat=_params['lat'],\n",
    "    lon=_params['lon'],\n",
    "    params=_steps['clasificador']['params'],\n",
    "    delete_partial_results=_steps['clasificador']['del_prev_result'],\n",
    "    to_tiff=True\n",
    ")\n",
    "\n",
    "\n",
    "entrenamiento>>clasificador\n",
    "workflow>>clasificador\n",
    "\n",
    "\n",
    "\n",
    "sensor_fin_ejecucion = CompressFileSensor(task_id='sensor_fin_ejecucion',poke_interval=60, soft_fail=True,mode='reschedule', queue='util', dag=dag) \n",
    "comprimir_resultados = PythonOperator(task_id='comprimir_resultados',provide_context=True,python_callable=other_utils.compress_results,queue='util',op_kwargs={'execID': args['execID']},dag=dag) \n",
    "sensor_fin_ejecucion >> comprimir_resultados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(product)\n",
    "print (\"Masking \" + product['name'])\n",
    "nodata=-9999\n",
    "validValues=set()\n",
    "if product['name']==\"LS7_ETM_LEDAPS\" or product['name'] == \"LS5_TM_LEDAPS\":\n",
    "    validValues=[66,68,130,132]\n",
    "elif product['name'] == \"LS8_OLI_LASRC\":\n",
    "    validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "else:\n",
    "    raise Exception(\"Este algoritmo sólo puede enmascarar LS7_ETM_LEDAPS, LS5_TM_LEDAPS o LS8_OLI_LASRC\")\n",
    "\n",
    "cloud_mask = np.isin(xarr0[\"pixel_qa\"].values, validValues)\n",
    "for band in product['bands']:\n",
    "    print(\"entra a enmascarar\")\n",
    "    xarr0[band].values = np.where(np.logical_and(xarr0.data_vars[band] != nodata, cloud_mask), xarr0.data_vars[band], -9999)\n",
    "output = xarr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob, os,sys\n",
    "\n",
    "output=None\n",
    "xarrs=xarrs.values()\n",
    "for _xarr in xarrs:\n",
    "    if (output is None):\n",
    "        output = _xarr\n",
    "    else:\n",
    "        output=output.combine_first(_xarr)\n",
    "\n",
    "#output=xr.auto_combine(list(xarrs))\n",
    "#output=xr.open_mfdataset(\"/source_storage/results/compuesto_de_medianas/compuesto-temporal-medianas-wf_1.0/*.nc\")\n",
    "#output=xr.merge(list(xarrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding=utf8\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "print (\"Compuesto temporal de medianas para \" + product['name'])\n",
    "print(xarr0)\n",
    "nodata=-9999\n",
    "medians = {}\n",
    "time_axis = list(xarr0.coords.keys()).index('time')\n",
    "\n",
    "\"\"\"\n",
    "\tModified by: Crhisitan Segura\n",
    "\tDate: 01 - dic - 2020\n",
    "\tDescription: used for cicatricez\n",
    "\"\"\"\n",
    "\n",
    "print(' lectura de xarr0')\n",
    "print(xarr0)\n",
    "print(type(xarr0))\n",
    "print('asignacion dem')\n",
    "\n",
    "dem=xarr0[\"dem\"][0].values\n",
    "\n",
    "print('finalizacion dem')\n",
    "\n",
    "print('bandas inicio')\n",
    "print(type(xarr0.data_vars))\n",
    "print(xarr0.data_vars)\n",
    "\n",
    "list_bandas=list(xarr0.data_vars)\n",
    "print('bandas anterior codigo')\n",
    "\n",
    "\n",
    "for band in list_bandas:\n",
    "    print('productbands')\n",
    "    #print(type('product['bands']'))\n",
    "    #print(product['bands'])\n",
    "    if band != 'pixel_qa':\n",
    "        datos = xarr0.data_vars[band].values\n",
    "        allNan = ~np.isnan(datos)\n",
    "\n",
    "        # Comentada por Aurelio (No soporta multi unidad)\n",
    "        #if normalized:\n",
    "        #    m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        if normalized:\n",
    "            m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "            st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "\n",
    "            # Expand m and st according with the data shape\n",
    "            # number of coords\n",
    "            coords_num = len(list(xarr0.coords.keys()))\n",
    "            l = [ x for x in range(coords_num) if x != time_axis]\n",
    "\n",
    "            m_new = m\n",
    "            st_new = st\n",
    "            for axis in l:\n",
    "                # If axis is 0  it is equivalent to x[np.newaxis,:]\n",
    "                # If axis is 1  it is equivalent to x[:,np.newaxis]\n",
    "                # And so on\n",
    "                m_new = np.expand_dims(m_new,axis=axis)\n",
    "                st_new = np.expand_dims(st_new,axis=axis)\n",
    "\n",
    "            print('Time axis',time_axis)\n",
    "            print('New axis',l)\n",
    "            print('m',m.shape)\n",
    "            print('st',st.shape)\n",
    "            print('st_new',st_new.shape)\n",
    "            print('m_new',m_new.shape)\n",
    "            datos=np.true_divide((datos-m_new), st_new)*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        medians[band] = np.nanmedian(datos, time_axis)\n",
    "        medians[band][np.sum(allNan, time_axis) < minValid] = -9999\n",
    "\n",
    "medians[\"dem\"]=dem\n",
    "medians[\"ndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"])\n",
    "medians[\"nbr\"]=np.true_divide(medians[\"swir2\"]-medians[\"nir\"],medians[\"swir2\"]+medians[\"nir\"])\n",
    "#Revisar NBR2 ccgs\n",
    "#medians[\"nbr2\"]=np.true_divide(medians[\"swir1\"]-medians[\"swir2\"],medians[\"swir1\"]+medians[\"swir2\"])\n",
    "#medians[\"ndmi\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])\n",
    "#medians[\"gndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"green\"],medians[\"nir\"]+medians[\"green\"])\n",
    "#medians[\"rvi\"]=np.true_divide(medians[\"nir\"],medians[\"red\"])\n",
    "#medians[\"nirv\"]=(medians[\"ndvi\"] * medians[\"nir\"])\n",
    "#medians[\"osavi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"]+0.16)\n",
    "\n",
    "print('medians_calculated')\n",
    "del datos\n",
    "\n",
    "# > **Asignación de coordenadas**\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in xarr0.coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, xarr0.coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=xarr0.coords[x]\n",
    "variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords) for k, v in medians.items()}\n",
    "output=xr.Dataset(variables, attrs={'crs':xarr0.crs})\n",
    "for x in output.coords:\n",
    "    output.coords[x].attrs[\"units\"]=xarr0.coords[x].units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,posixpath\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gdal\n",
    "import zipfile\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#parametros:\n",
    "#xarr0: Mosaico del compuesto de medianas\n",
    "#bands: Las bandas a utilizar\n",
    "#train_data_path: Ubicación de los shape files .shp\n",
    "\n",
    "'''\n",
    "\tCode edited by Crhistian Segura\n",
    " \tDate: 17-nov-2020\n",
    "\tModif: add train test split and statistics for validation\n",
    "'''\n",
    "\n",
    "def enmascarar_entrenamiento(vector_data_path, cols, rows, geo_transform, projection, target_value=1):\n",
    "    data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "    layer = data_source.GetLayer(0)\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    gdal.RasterizeLayer(target_ds, [1], layer, burn_values=[target_value])\n",
    "    return target_ds\n",
    "\n",
    "def rasterizar_entrenamiento(file_paths, rows, cols, geo_transform, projection):\n",
    "    labeled_pixels = np.zeros((rows, cols))\n",
    "    for i, path in enumerate(file_paths):\n",
    "        label = i+1\n",
    "        print  (\"label\")\n",
    "        print (label)\n",
    "        ds = enmascarar_entrenamiento(path, cols, rows, geo_transform, projection, target_value=label)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        labeled_pixels += band.ReadAsArray()\n",
    "        print  (\"labeled_pixels\")\n",
    "        print (labeled_pixels)\n",
    "        #ds = None\n",
    "    return labeled_pixels\n",
    "\n",
    "# The trainning data must be in a zip folder.\n",
    "train_zip_file_name  = [file_name for file_name in os.listdir(train_data_path) if file_name.endswith('.zip')][0]\n",
    "train_zip_file_path = os.path.join(train_data_path,train_zip_file_name)\n",
    "train_folder_path = train_zip_file_path.replace('.zip','')\n",
    "\n",
    "print('train_zip_file_path',train_zip_file_path)\n",
    "print('train_folder_path',train_folder_path)\n",
    "\n",
    "zip_file = zipfile.ZipFile(train_zip_file_path)\n",
    "zip_file.extractall(train_data_path)\n",
    "zip_file.close()\n",
    "\n",
    "#files = [f for f in os.listdir(train_data_path) if f.endswith('.shp')]\n",
    "files = [f for f in os.listdir(train_folder_path) if f.endswith('.shp')]\n",
    "classes = [f.split('.')[0] for f in files]\n",
    "print(classes)\n",
    "#shapefiles = [os.path.join(train_data_path, f) for f in files if f.endswith('.shp')]\n",
    "shapefiles = [os.path.join(train_folder_path, f) for f in files if f.endswith('.shp')]\n",
    "\n",
    "rows, cols = xarr0[product['bands'][0]].shape\n",
    "_coords=xarr0.coords\n",
    "\n",
    "\n",
    "#(originX, pixelWidth, 0, originY, 0, pixelHeight)\n",
    "geo_transform=(_coords[\"longitude\"].values[0], 0.000269995,0, _coords[\"latitude\"].values[0],0,-0.000271302)\n",
    "proj = xarr0.crs.crs_wkt\n",
    "\n",
    "print('shapefile',shapefiles)\n",
    "\n",
    "labeled_pixels = rasterizar_entrenamiento(shapefiles, rows, cols, geo_transform, proj)\n",
    "\n",
    "is_train = np.nonzero(labeled_pixels)\n",
    "training_labels = labeled_pixels[is_train]\n",
    "bands_data=[]\n",
    "#reordenar bandas\n",
    "xarr0 = xarr0[['swir2','nir','blue','nbr','ndvi']]\n",
    "\n",
    "print(xarr0)\n",
    "#print(\"aaaaaaaaaa\")\n",
    "#print(df)\n",
    "\n",
    "\n",
    "for band in ['swir2','nir','blue','nbr','ndvi']:\n",
    "    # pixel_qa is removed from xarr0 by Compuesto Temporal de Medianas\n",
    "    if band != 'pixel_qa':\n",
    "        bands_data.append(xarr0[band])\n",
    "bands_data = np.dstack(bands_data)\n",
    "training_samples = bands_data[is_train]\n",
    "print('training_samples')\n",
    "#print(training_samples.shape())\n",
    "\n",
    "rows, cols, n_bands = bands_data.shape\n",
    "\n",
    "np.isfinite(training_samples)\n",
    "_msk=np.sum(np.isfinite(training_samples),1)>1\n",
    "training_samples= training_samples[_msk,:]\n",
    "training_labels=training_labels[_msk]\n",
    "\n",
    "#mascara valores nan por valor no data\n",
    "mask_nan=np.isnan(training_samples)\n",
    "training_samples[mask_nan]=-9999\n",
    "print('training_samples')\n",
    "print(training_samples)\n",
    "#print(training_samples.shape())\n",
    "\n",
    "print('training_labels')\n",
    "print(training_labels)\n",
    "#print(training_labels.shape())\n",
    "\n",
    "# Split train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_samples, training_labels, test_size=0.3)\n",
    "\n",
    "print(f'Train X {len(X_train)/len(training_samples)*100:.2f}%')\n",
    "print(f'Train Y {len(y_train)/len(training_samples)*100:.2f}%')\n",
    "print(f'Test  X {len(X_test )/len(training_samples)*100:.2f}%')\n",
    "print(f'Test  Y {len(y_test )/len(training_samples)*100:.2f}%')\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.hist(y_train,bins=np.arange(1,len(np.unique(training_labels))+2)-0.5)\n",
    "#plt.rcParams['figure.facecolor'] = 'white'\n",
    "#plt.savefig(posixpath.join(folder,'Histogram_tain_data.png'))\n",
    "print('Se usaran 500 arboles!!')\n",
    "classifier = RandomForestClassifier(n_jobs=-1, n_estimators=500, verbose=1)\n",
    "\n",
    "print('trainning samples',X_train)\n",
    "print('trainning labels',y_train)\n",
    "#classifier = RandomForestClassifier(n_jobs=-1, n_estimators=50, verbose=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Calculo de y_pred\n",
    "print('Estimar y con datos de entrada')\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculo de matrix de confusion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, precision_score\n",
    "\n",
    "mconf = confusion_matrix(y_test,y_pred)\n",
    "# Calculo de kappa score\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "# Calculo de precision score\n",
    "prec = precision_score(y_test, y_pred,average = 'weighted')\n",
    "\n",
    "# Save metrics to file\n",
    "with open(posixpath.join(folder+'metrics.txt'),'w') as file_metrics:\n",
    "    print(f'matriz de confusion: {mconf}')\n",
    "    print(f'kappa score: {kappa}')\n",
    "    print(f'precision score (weighted): {prec}')\n",
    "    file_metrics.write('matriz de confusion: \\n'+str(mconf))\n",
    "    file_metrics.write('\\nkappa score: '+str(kappa))\n",
    "    file_metrics.write('\\nprecision score (weighted): '+str(prec))\n",
    "\n",
    "# write shapefiles list\n",
    "file = open(folder+\"shapefiles_list.txt\", \"w\")\n",
    "file.write(\"shapefiles list = \" + \"\\n\".join(shapefiles))\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "outputxcom=posixpath.join(folder,'modelo_random_forest.pkl')\n",
    "with open(outputxcom, 'wb') as fid:\n",
    "    print('output',classifier)\n",
    "    joblib.dump(classifier, fid)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "# Preprocesar:\n",
    "nmed=None\n",
    "nan_mask=None\n",
    "xarrs=list(xarrs.values())\n",
    "print(type(xarrs))\n",
    "medians1 = xarrs[0]\n",
    "print(type(medians1))\n",
    "print(\"medianas\")\n",
    "bands_data = []\n",
    "\n",
    "print('medians1.datavars',medians1.data_vars.keys())\n",
    "\n",
    "#rows, cols = xarr0[product['bands'][0]].shape\n",
    "\n",
    "#print('rows',rows)\n",
    "#print('cols',cols)\n",
    "\n",
    "\n",
    "#for band in product['bands']:\n",
    "#    # pixel_qa is removed from xarr0 by Compuesto Temporal de Medianas\n",
    "#    if band != 'pixel_qa':\n",
    "#        bands_data.append(xarr0[band])\n",
    "#bands_data = np.dstack(bands_data)\n",
    "\n",
    "###test 2\n",
    "bands_data=[]\n",
    "\n",
    "bands2=list(medians1.data_vars.keys())\n",
    "print('bands2',bands2)\n",
    "#print('mediansband',medians1[band])\n",
    "\n",
    "bands2=['swir2','nir','blue','nbr','ndvi']\n",
    "for band in bands2:\n",
    "\tif band != 'pixel_qa':\n",
    "   \t bands_data.append(medians1[band])\n",
    "bands_data = np.dstack(bands_data)\n",
    "\n",
    "print('bands_data_test2',bands_data)\n",
    "\n",
    "\n",
    "rows, cols, n_bands = bands_data.shape\n",
    "\n",
    "print('rows',rows)\n",
    "print('cols',cols)\n",
    "\n",
    "print('n_bands',n_bands)\n",
    "\n",
    "\n",
    "\n",
    "print('fin test2')\n",
    "\n",
    "print('inicio test3')\n",
    "\n",
    "n_samples = rows*cols\n",
    "flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "#mascara valores nan por valor no data\n",
    "mask_nan=np.isnan(flat_pixels)\n",
    "flat_pixels[mask_nan]=-9999\n",
    "#result = bagging_clf.predict(flat_pixels)\n",
    "#classification = result.reshape((rows, cols))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('fin test3')\n",
    "\n",
    "#for band in medians1.data_vars.keys():\n",
    "\n",
    "#    if band == \"crs\":\n",
    "#        continue\n",
    "#    b=np.ravel(medians1.data_vars[band].values)\n",
    "#    if nan_mask is None:\n",
    "#        nan_mask=np.isnan(b)\n",
    "#    else:\n",
    "#        nan_mask=np.logical_or(nan_mask, np.isnan(medians1.data_vars[band].values.ravel()))\n",
    "#    b[np.isnan(b)]=np.nanmedian(b)\n",
    "#    if nmed is None:\n",
    "#        sp=medians1.data_vars[band].values.shape\n",
    "#        nmed=b\n",
    "#    else:\n",
    "#        nmed=np.vstack((nmed,b))\n",
    "\n",
    "#print('mediansdata',\n",
    "#\tsp=medians1.data_vars[band].values.shape\n",
    "#\tbands_data.append(medians1[band])\n",
    "#bands_data = np.dstack(bands_data)\n",
    "\n",
    "#print('medians1.datavars',sp)\n",
    "\n",
    "#rows, cols = sp\n",
    "\n",
    "#print('rows',rows)\n",
    "#print('cols',cols)\n",
    "\n",
    "#print('bands',bands)\n",
    "\n",
    "#for band in bands: \n",
    "#    bands_data.append(medians1[band])\n",
    "#bands_data = np.dstack(bands_data)\n",
    "\n",
    "#print('bands',bands_data)\n",
    "\n",
    "#n_samples = rows*cols\n",
    "#flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "#mascara valores nan por valor no data\n",
    "#mask_nan=np.isnan(flat_pixels)\n",
    "#flat_pixels[mask_nan]=-9999\n",
    "#result = bagging_clf.predict(flat_pixels)\n",
    "#classification = result.reshape((rows, cols))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"modelo\")\n",
    "\n",
    "model = None\n",
    "for file in other_files:\n",
    "    if file.endswith(\".pkl\"):\n",
    "        model = file\n",
    "        break\n",
    "if model is None:\n",
    "    raise \"Debería haber un modelo en la carpeta \" + modelos\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    eclf = joblib.load(model)\n",
    "    print(eclf)\n",
    "\n",
    "print(eclf)\n",
    "print(\"clasificacion final\")\n",
    "result = eclf.predict(flat_pixels)\n",
    "result = result.reshape((rows, cols))\n",
    "print(\"fin funcion de clasificacion\")\n",
    "\n",
    "#result = bagging_clf.predict(nmed.T)\n",
    "#result = result.reshape(sp)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "coordenadas = []\n",
    "dimensiones = []\n",
    "xcords = {}\n",
    "for coordenada in xarrs[0].coords:\n",
    "    if (coordenada != 'time'):\n",
    "        coordenadas.append((coordenada, xarrs[0].coords[coordenada]))\n",
    "        dimensiones.append(coordenada)\n",
    "        xcords[coordenada] = xarrs[0].coords[coordenada]\n",
    "\n",
    "valores = {\"classified\": xr.DataArray(result, dims=dimensiones, coords=coordenadas)}\n",
    "#array = xr.DataArray(result, dims=dimensiones, coords=coordenadas)\n",
    "#array.astype('float32')\n",
    "#valores = {\"classified\": array}\n",
    "\n",
    "output = xr.Dataset(valores, attrs={'crs': xarrs[0].crs})\n",
    "for coordenada in output.coords:\n",
    "    output.coords[coordenada].attrs[\"units\"] = xarrs[0].coords[coordenada].units\n",
    "\n",
    "classified = output.classified\n",
    "classified.values = classified.values.astype('float32')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
