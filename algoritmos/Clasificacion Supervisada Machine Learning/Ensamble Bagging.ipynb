{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble Bagging workflow and notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators import CompressFileSensor\n",
    "from cdcol_utils import other_utils\n",
    "import airflow\n",
    "from airflow.models import DAG\n",
    "from airflow.operators import CDColQueryOperator, CDColFromFileOperator, CDColReduceOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from cdcol_utils import dag_utils, queue_utils, other_utils\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "from datetime import timedelta\n",
    "from pprint import pprint\n",
    "\n",
    "_params = {'modelos': '/web_storage/downloads/6863', 'minValid': 1, 'normalized': False, 'lat': (2, 3), 'lon': (-74, -73), 'products': [{'name': 'LS7_ETM_LEDAPS', 'bands': ['swir1', 'red', 'nir', 'green', 'blue', 'swir2', 'pixel_qa']}, {'name': 'LS8_OLI_LASRC', 'bands': ['swir1', 'red', 'nir', 'green', 'blue', 'swir2', 'pixel_qa']}], 'time_ranges': [('2020-01-01', '2020-06-30'), ('2020-01-01', '2020-06-30')], 'execID': 'exec_6863', 'elimina_resultados_anteriores': True, 'genera_mosaico': True, 'owner': 'API-REST'}\n",
    "\n",
    "# definir unidades FNF, y DEM por defecto (una sola banda)\n",
    "_params['products'].append({'name': 'DEM_Mosaico', 'bands': ['dem']})\n",
    "_params['products'].append({'name': 'FNF_COL_UTM', 'bands': ['fnf_mask']})\n",
    "\n",
    "# Definir periodo de tiempo DEM\n",
    "_params['time_ranges'] = [('2013-01-01','2013-12-31')] + _params['time_ranges']\n",
    "\n",
    "# sort params products by name\n",
    "_params['products'].sort(key = lambda d: d['name'])\n",
    "\n",
    "\"\"\"\n",
    "Templeate modified my Crhsitian Segura\n",
    "27-oct-2020\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "_params = {'minValid': 1, 'normalized': False,\n",
    "    'modelos': '/web_storage/downloads/3625',\n",
    "    'lat': (10, 11),\n",
    "    'lon': (-75, -74),\n",
    "    'bands': [\"red\", \"nir\", \"swir1\", \"swir2\"],\n",
    "    #'minValid': 1,\n",
    "    'products': [{'name': 'LS7_ETM_LEDAPS_MOSAIC', 'bands': ['swir2', 'nir', 'red', 'swir1']},{'name': 'DEM_Mosaico', 'bands': ['dem']},{'name': 'FNF_COL_UTM', 'bands': ['fnf_mask']}],\n",
    "    'time_ranges': [('2016-01-01', '2016-12-31'),('2013-01-01', '2013-12-31'),('2017-01-01', '2017-12-31')],\n",
    "    'execID': \"ctm_b_01\",\n",
    "    'elimina_resultados_anteriores': True,\n",
    "    'genera_mosaico': True,\n",
    "    #'genera_geotiff': True,\n",
    "    'owner': 'cubo',\n",
    "    'normalized': False\n",
    "}\n",
    "\"\"\"\n",
    "_steps = {\n",
    "    'mascara': {\n",
    "        'algorithm': \"mascara-landsat\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(input_type='multi_temporal', time_range=_params['time_ranges'][2]),\n",
    "        'params': {'bands': _params['products'][2]['bands']},\n",
    "    },\n",
    "    'consulta': {\n",
    "        'algorithm': \"mascara-landsat\",\n",
    "        'version': '2.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_temporal_area',\n",
    "            time_range=_params['time_ranges'][2],\n",
    "            lat=_params['lat'], lon=_params['lon']),\n",
    "        'params': {'bands': _params['products'][2]['bands']},\n",
    "    },\n",
    "    'reduccion': {\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': 'airflow_xlarge',\n",
    "        'params': {'bands': _params['products'][2]['bands']},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'medianas': {\n",
    "        'algorithm': \"compuesto-temporal-medianas-indices-wf\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(input_type='multi_temporal_unidad',\n",
    "                                          time_range=_params['time_ranges'][2],\n",
    "                                          unidades=len(_params['products'])),\n",
    "        'params': {\n",
    "            'bands': _params['products'][2]['bands'],\n",
    "            'minValid': _params['minValid'],\n",
    "            'normalized': False,\n",
    "        },\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'medianas_dem': {\n",
    "        'algorithm': \"compuesto-temporal-medianas-indices-wf\",\n",
    "        'version': '3.0',\n",
    "        'queue': queue_utils.assign_queue(input_type='multi_temporal_unidad',\n",
    "                                          time_range=_params['time_ranges'][2],\n",
    "                                          unidades=len(_params['products'])),\n",
    "        'params': {\n",
    "            'bands': _params['products'][2]['bands'],\n",
    "            'minValid': _params['minValid'],\n",
    "            'normalized': False,\n",
    "        },\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'mosaico': {\n",
    "        'algorithm': \"joiner\",\n",
    "        'version': '1.0',\n",
    "        'queue': 'airflow_xlarge',\n",
    "        'params': {},\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "    'entrenamiento': {\n",
    "        'algorithm': \"ensemble-training\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(\n",
    "            input_type='multi_area',\n",
    "            lat=_params['lat'],\n",
    "            lon=_params['lon']\n",
    "        ),\n",
    "        'params': {\n",
    "            'bands': _params['products'][2]['bands'],\n",
    "            'train_data_path': _params['modelos']\n",
    "        },\n",
    "        'del_prev_result': False,\n",
    "    },\n",
    "    'clasificador': {\n",
    "        'algorithm': \"clasificador-ensemble-wf\",\n",
    "        'version': '1.0',\n",
    "        'queue': 'airflow_xlarge',\n",
    "        'params': {\n",
    "            'bands': _params['products'][0]['bands'],\n",
    "            'modelos': _params['modelos']\n",
    "        },\n",
    "        'del_prev_result': _params['elimina_resultados_anteriores'],\n",
    "    },\n",
    "     'mascara_fnf': {\n",
    "        'algorithm': \"mascara_fnf\",\n",
    "        'version': '1.0',\n",
    "        'queue': 'airflow_medium',\n",
    "        'params': {\n",
    "            'bands': _params['products'][1]['bands'],\n",
    "            'modelos': _params['modelos']\n",
    "        },\n",
    "        'del_prev_result': True,\n",
    "    },\n",
    "    'geotiff': {\n",
    "        'algorithm': \"generate-geotiff\",\n",
    "        'version': '1.0',\n",
    "        'queue': queue_utils.assign_queue(input_type='multi_area', lat=_params['lat'], lon=_params['lon']),\n",
    "        'params': {},\n",
    "        'del_prev_result': False,\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'owner': _params['owner'],\n",
    "    'start_date': airflow.utils.dates.days_ago(2),\n",
    "    'execID':_params['execID'],\n",
    "    'product': _params['products'][2]\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=args['execID'], default_args=args,\n",
    "    schedule_interval=None,\n",
    "    dagrun_timeout=timedelta(minutes=120))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mascara_0 = dag_utils.queryMapByTile(\n",
    "    lat=_params['lat'], \n",
    "    lon=_params['lon'],\n",
    "    time_ranges=_params['time_ranges'][2],\n",
    "    algorithm=_steps['mascara']['algorithm'],\n",
    "    version=_steps['mascara']['version'],\n",
    "    product=_params['products'][2],\n",
    "    params=_steps['mascara']['params'],\n",
    "    queue=_steps['mascara']['queue'],\n",
    "    dag=dag,\n",
    "    task_id=\"mascara_\" + _params['products'][2]['name']\n",
    ")\n",
    "\n",
    "if len(_params['products']) > 3:\n",
    "    mascara_1 = dag_utils.queryMapByTile(\n",
    "        lat=_params['lat'],\n",
    "        lon=_params['lon'],\n",
    "        time_ranges=_params['time_ranges'][2],\n",
    "        algorithm=_steps['mascara']['algorithm'],\n",
    "        version=_steps['mascara']['version'],\n",
    "        product=_params['products'][3],\n",
    "        params=_steps['mascara']['params'],\n",
    "        queue=_steps['mascara']['queue'],\n",
    "        dag=dag,\n",
    "        task_id=\"mascara_\" + _params['products'][3]['name']\n",
    "    )\n",
    "\n",
    "    reduccion_lansat = dag_utils.reduceByTile(\n",
    "        mascara_0 + mascara_1,\n",
    "        algorithm=_steps['reduccion']['algorithm'],\n",
    "        version=_steps['reduccion']['version'],\n",
    "        queue=_steps['reduccion']['queue'],\n",
    "        product=_params['products'][2],\n",
    "        dag=dag, task_id=\"joined\",\n",
    "        delete_partial_results=_steps['reduccion']['del_prev_result'],\n",
    "        params=_steps['reduccion']['params'], \n",
    "    )\n",
    "else:\n",
    "    reduccion_lansat = mascara_0\n",
    "\n",
    "\n",
    "medianas = dag_utils.IdentityMap(\n",
    "                                reduccion_lansat,\n",
    "                                product=_params['products'][2],\n",
    "                                algorithm=_steps['medianas']['algorithm'],\n",
    "                                version=_steps['medianas']['version'],\n",
    "                                task_id=\"medianas\",\n",
    "                                queue=_steps['medianas']['queue'],\n",
    "                                dag=dag,\n",
    "                                delete_partial_results=_steps['medianas']['del_prev_result'],\n",
    "                                params=_steps['medianas']['params']\n",
    ")\n",
    "\n",
    "\n",
    "mascara_dem_mosaic = dag_utils.queryMapByTile(lat=_params['lat'], lon=_params['lon'],\n",
    "                                       time_ranges=_params['time_ranges'][0],\n",
    "                                       algorithm=_steps['consulta']['algorithm'],\n",
    "                                       version=_steps['consulta']['version'],\n",
    "                                       product=_params['products'][0],\n",
    "                                       params=_steps['consulta']['params'],\n",
    "                                       queue=_steps['consulta']['queue'], dag=dag,\n",
    "                                       task_id=\"consulta_referencia_\" + _params['products'][0]['name'])\n",
    "\n",
    "\n",
    "mascara_fnf_mosaic = CDColQueryOperator(lat=_params['lat'], lon=_params['lon'],\n",
    "                                        time_ranges=_params['time_ranges'][1],\n",
    "                                        algorithm=_steps['consulta']['algorithm'],\n",
    "                                        version=_steps['consulta']['version'],\n",
    "                                        product=_params['products'][1],\n",
    "                                        params=_steps['consulta']['params'],\n",
    "                                        queue=_steps['consulta']['queue'],\n",
    "                                        dag=dag,\n",
    "                                        task_id=\"consulta_referencia_\" + _params['products'][1]['name'])\n",
    "\n",
    "reduccion = dag_utils.reduceByTile(\n",
    "                                    medianas + mascara_dem_mosaic,\n",
    "                                    product=_params['products'][2],\n",
    "                                    algorithm=_steps['reduccion']['algorithm'],\n",
    "                                    version=_steps['reduccion']['version'],\n",
    "                                    queue=_steps['reduccion']['queue'],\n",
    "                                    dag=dag, task_id=\"joined_2\",\n",
    "                                    delete_partial_results=_steps['reduccion']['del_prev_result'],\n",
    "                                    params=_steps['reduccion']['params'],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "medianas_dem = dag_utils.IdentityMap(\n",
    "                                reduccion,\n",
    "                                product=_params['products'][2],\n",
    "                                algorithm=_steps['medianas_dem']['algorithm'],\n",
    "                                version=_steps['medianas_dem']['version'],\n",
    "                                task_id=\"medianas_dem\",\n",
    "                                queue=_steps['medianas_dem']['queue'],\n",
    "                                dag=dag,\n",
    "                                delete_partial_results=_steps['medianas_dem']['del_prev_result'],\n",
    "                                params=_steps['medianas_dem']['params']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mosaico = dag_utils.OneReduce(medianas_dem, task_id=\"mosaico_consulta\",\n",
    "                              algorithm=_steps['mosaico']['algorithm'],\n",
    "                              version=_steps['mosaico']['version'],\n",
    "                              queue=_steps['mosaico']['queue'],\n",
    "                              delete_partial_results=_steps['mosaico']['del_prev_result'],\n",
    "                              trigger_rule=TriggerRule.NONE_FAILED, dag=dag)\n",
    "\n",
    "entrenamiento = dag_utils.IdentityMap(\n",
    "                                mosaico,\n",
    "                                algorithm=_steps['entrenamiento']['algorithm'],\n",
    "                                version=_steps['entrenamiento']['version'],\n",
    "                                task_id=\"entrenamiento\",\n",
    "                                queue=_steps['entrenamiento']['queue'],\n",
    "                                dag=dag,\n",
    "                                delete_partial_results=_steps['entrenamiento']['del_prev_result'],\n",
    "                                params=_steps['entrenamiento']['params']\n",
    ")\n",
    "\n",
    "clasificador = CDColReduceOperator(\n",
    "                                task_id=\"clasificador_generico\",\n",
    "                                algorithm=_steps['clasificador']['algorithm'],\n",
    "                                version=_steps['clasificador']['version'],\n",
    "                                queue=_steps['clasificador']['queue'],\n",
    "                                dag=dag,\n",
    "                                lat=_params['lat'],\n",
    "                                lon=_params['lon'],\n",
    "                                params=_steps['clasificador']['params'],\n",
    "                                delete_partial_results=_steps['clasificador']['del_prev_result'],\n",
    "\n",
    ")\n",
    "\n",
    "mascara_fnf = CDColReduceOperator(algorithm=_steps['mascara_fnf']['algorithm'],\n",
    "                                       version=_steps['mascara_fnf']['version'],\n",
    "                                       queue=_steps['mascara_fnf']['queue'],\n",
    "                                       params=_steps['mascara_fnf']['params'],\n",
    "                                       delete_partial_results=_steps['mascara_fnf']['del_prev_result'],\n",
    "                                       dag=dag, task_id=\"clasificacion_final\", to_tiff=True)\n",
    "\n",
    "\n",
    "entrenamiento>>clasificador\n",
    "mosaico>>clasificador\n",
    "\n",
    "\n",
    "\n",
    "clasificador>>mascara_fnf\n",
    "mascara_fnf_mosaic>>mascara_fnf\n",
    "\n",
    "sensor_fin_ejecucion = CompressFileSensor(task_id='sensor_fin_ejecucion',poke_interval=60, soft_fail=True,mode='reschedule', queue='util', dag=dag) \n",
    "comprimir_resultados = PythonOperator(task_id='comprimir_resultados',provide_context=True,python_callable=other_utils.compress_results,queue='util',op_kwargs={'execID': args['execID']},dag=dag) \n",
    "sensor_fin_ejecucion >> comprimir_resultados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(product)\n",
    "print (\"Masking \" + product['name'])\n",
    "nodata=-9999\n",
    "validValues=set()\n",
    "if product['name']==\"LS7_ETM_LEDAPS\" or product['name'] == \"LS5_TM_LEDAPS\":\n",
    "    validValues=[66,68,130,132]\n",
    "elif product['name'] == \"LS8_OLI_LASRC\":\n",
    "    validValues=[322, 386, 834, 898, 1346, 324, 388, 836, 900, 1348]\n",
    "else:\n",
    "    raise Exception(\"Este algoritmo sólo puede enmascarar LS7_ETM_LEDAPS, LS5_TM_LEDAPS o LS8_OLI_LASRC\")\n",
    "\n",
    "cloud_mask = np.isin(xarr0[\"pixel_qa\"].values, validValues)\n",
    "for band in product['bands']:\n",
    "    print(\"entra a enmascarar\")\n",
    "    xarr0[band].values = np.where(np.logical_and(xarr0.data_vars[band] != nodata, cloud_mask), xarr0.data_vars[band], -9999)\n",
    "output = xarr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob, os,sys\n",
    "\n",
    "output=None\n",
    "xarrs=xarrs.values()\n",
    "for _xarr in xarrs:\n",
    "    if (output is None):\n",
    "        output = _xarr\n",
    "    else:\n",
    "        output=output.combine_first(_xarr)\n",
    "\n",
    "#output=xr.auto_combine(list(xarrs))\n",
    "#output=xr.open_mfdataset(\"/source_storage/results/compuesto_de_medianas/compuesto-temporal-medianas-wf_1.0/*.nc\")\n",
    "#output=xr.merge(list(xarrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding=utf8\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "print (\"Compuesto temporal de medianas para \" + product['name'])\n",
    "print(xarr0)\n",
    "nodata=-9999\n",
    "medians = {}\n",
    "time_axis = list(xarr0.coords.keys()).index('time')\n",
    "for band in product['bands']:\n",
    "    print(product['bands'])\n",
    "    if band != 'pixel_qa':\n",
    "        datos = xarr0.data_vars[band].values\n",
    "        allNan = ~np.isnan(datos)\n",
    "\n",
    "        # Comentada por Aurelio (No soporta multi unidad)\n",
    "        #if normalized:\n",
    "        #    m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        if normalized:\n",
    "            m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "            st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "\n",
    "            # Expand m and st according with the data shape\n",
    "            # number of coords\n",
    "            coords_num = len(list(xarr0.coords.keys()))\n",
    "            l = [ x for x in range(coords_num) if x != time_axis]\n",
    "\n",
    "            m_new = m\n",
    "            st_new = st\n",
    "            for axis in l:\n",
    "                # If axis is 0  it is equivalent to x[np.newaxis,:]\n",
    "                # If axis is 1  it is equivalent to x[:,np.newaxis]\n",
    "                # And so on\n",
    "                m_new = np.expand_dims(m_new,axis=axis)\n",
    "                st_new = np.expand_dims(st_new,axis=axis)\n",
    "\n",
    "            print('Time axis',time_axis)\n",
    "            print('New axis',l)\n",
    "            print('m',m.shape)\n",
    "            print('st',st.shape)\n",
    "            print('st_new',st_new.shape)\n",
    "            print('m_new',m_new.shape)\n",
    "            datos=np.true_divide((datos-m_new), st_new)*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        medians[band] = np.nanmedian(datos, time_axis)\n",
    "        medians[band][np.sum(allNan, time_axis) < minValid] = -9999\n",
    "\n",
    "medians[\"ndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"])\n",
    "medians[\"nbr\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])\n",
    "medians[\"nbr2\"]=np.true_divide(medians[\"swir1\"]-medians[\"swir2\"],medians[\"swir1\"]+medians[\"swir2\"])\n",
    "medians[\"ndmi\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])\n",
    "#medians[\"gndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"green\"],medians[\"nir\"]+medians[\"green\"])\n",
    "medians[\"rvi\"]=np.true_divide(medians[\"nir\"],medians[\"red\"])\n",
    "medians[\"nirv\"]=(medians[\"ndvi\"] * medians[\"nir\"])\n",
    "medians[\"osavi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"]+0.16)\n",
    "\n",
    "\n",
    "print('medians_calculated')\n",
    "del datos\n",
    "\n",
    "# > **Asignación de coordenadas**\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in xarr0.coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, xarr0.coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=xarr0.coords[x]\n",
    "variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords) for k, v in medians.items()}\n",
    "output=xr.Dataset(variables, attrs={'crs':xarr0.crs})\n",
    "for x in output.coords:\n",
    "    output.coords[x].attrs[\"units\"]=xarr0.coords[x].units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding=utf8\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "print (\"Compuesto temporal de medianas para \" + product['name'])\n",
    "print(xarr0)\n",
    "nodata=-9999\n",
    "medians = {}\n",
    "time_axis = list(xarr0.coords.keys()).index('time')\n",
    "\n",
    "print(' lectura de xarr0')\n",
    "print(xarr0)\n",
    "print(type(xarr0))\n",
    "print('asignacion dem')\n",
    "\n",
    "dem=xarr0[\"dem\"][0].values\n",
    "\n",
    "print('finalizacion dem')\n",
    "\n",
    "print('bandas inicio')\n",
    "print(type(xarr0.data_vars))\n",
    "print(xarr0.data_vars)\n",
    "\n",
    "list_bandas=list(xarr0.data_vars)\n",
    "print('bandas anterior codigo')\n",
    "\n",
    "\n",
    "for band in list_bandas:\n",
    "    print('productbands')\n",
    "    #print(type('product['bands']'))\n",
    "    #print(product['bands'])\n",
    "    if band != 'pixel_qa':\n",
    "        datos = xarr0.data_vars[band].values\n",
    "        allNan = ~np.isnan(datos)\n",
    "\n",
    "        # Comentada por Aurelio (No soporta multi unidad)\n",
    "        #if normalized:\n",
    "        #    m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "        #    datos=np.true_divide((datos-m[:,np.newaxis,np.newaxis]), st[:,np.newaxis,np.newaxis])*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        if normalized:\n",
    "            m=np.nanmean(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "            st=np.nanstd(datos.reshape((datos.shape[time_axis],-1)), axis=1)\n",
    "\n",
    "            # Expand m and st according with the data shape\n",
    "            # number of coords\n",
    "            coords_num = len(list(xarr0.coords.keys()))\n",
    "            l = [ x for x in range(coords_num) if x != time_axis]\n",
    "\n",
    "            m_new = m\n",
    "            st_new = st\n",
    "            for axis in l:\n",
    "                # If axis is 0  it is equivalent to x[np.newaxis,:]\n",
    "                # If axis is 1  it is equivalent to x[:,np.newaxis]\n",
    "                # And so on\n",
    "                m_new = np.expand_dims(m_new,axis=axis)\n",
    "                st_new = np.expand_dims(st_new,axis=axis)\n",
    "\n",
    "            print('Time axis',time_axis)\n",
    "            print('New axis',l)\n",
    "            print('m',m.shape)\n",
    "            print('st',st.shape)\n",
    "            print('st_new',st_new.shape)\n",
    "            print('m_new',m_new.shape)\n",
    "            datos=np.true_divide((datos-m_new), st_new)*np.nanmean(st)+np.nanmean(m)\n",
    "\n",
    "        medians[band] = np.nanmedian(datos, time_axis)\n",
    "        medians[band][np.sum(allNan, time_axis) < minValid] = -9999\n",
    "\n",
    "medians[\"dem\"]=dem\n",
    "medians[\"ndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"])\n",
    "medians[\"nbr\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])\n",
    "#Revisar NBR2 ccgs\n",
    "medians[\"nbr2\"]=np.true_divide(medians[\"swir1\"]-medians[\"swir2\"],medians[\"swir1\"]+medians[\"swir2\"])\n",
    "medians[\"ndmi\"]=np.true_divide(medians[\"nir\"]-medians[\"swir1\"],medians[\"nir\"]+medians[\"swir1\"])\n",
    "#medians[\"gndvi\"]=np.true_divide(medians[\"nir\"]-medians[\"green\"],medians[\"nir\"]+medians[\"green\"])\n",
    "medians[\"rvi\"]=np.true_divide(medians[\"nir\"],medians[\"red\"])\n",
    "medians[\"nirv\"]=(medians[\"ndvi\"] * medians[\"nir\"])\n",
    "#medians[\"osavi\"]=np.true_divide(medians[\"nir\"]-medians[\"red\"],medians[\"nir\"]+medians[\"red\"]+0.16)\n",
    "\n",
    "\n",
    "print('medians_calculated')\n",
    "del datos\n",
    "\n",
    "# > **Asignación de coordenadas**\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in xarr0.coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, xarr0.coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=xarr0.coords[x]\n",
    "variables ={k: xr.DataArray(v, dims=xdims,coords=ncoords) for k, v in medians.items()}\n",
    "output=xr.Dataset(variables, attrs={'crs':xarr0.crs})\n",
    "for x in output.coords:\n",
    "    output.coords[x].attrs[\"units\"]=xarr0.coords[x].units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,posixpath\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gdal\n",
    "import zipfile\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#parametros:\n",
    "#xarr0: Mosaico del compuesto de medianas\n",
    "#bands: Las bandas a utilizar\n",
    "#train_data_path: Ubicación de los shape files .shp\n",
    "\n",
    "def enmascarar_entrenamiento(vector_data_path, cols, rows, geo_transform, projection, target_value=1):\n",
    "    data_source = gdal.OpenEx(vector_data_path, gdal.OF_VECTOR)\n",
    "    layer = data_source.GetLayer(0)\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    target_ds = driver.Create('', cols, rows, 1, gdal.GDT_UInt16)\n",
    "    target_ds.SetGeoTransform(geo_transform)\n",
    "    target_ds.SetProjection(projection)\n",
    "    gdal.RasterizeLayer(target_ds, [1], layer, burn_values=[target_value])\n",
    "    return target_ds\n",
    "\n",
    "def rasterizar_entrenamiento(file_paths, rows, cols, geo_transform, projection):\n",
    "    labeled_pixels = np.zeros((rows, cols))\n",
    "    for i, path in enumerate(file_paths):\n",
    "        label = i+1\n",
    "        print  (\"label\")\n",
    "        print (label)\n",
    "        ds = enmascarar_entrenamiento(path, cols, rows, geo_transform, projection, target_value=label)\n",
    "        band = ds.GetRasterBand(1)\n",
    "        labeled_pixels += band.ReadAsArray()\n",
    "        print  (\"labeled_pixels\")\n",
    "        print (labeled_pixels)\n",
    "        #ds = None\n",
    "    return labeled_pixels\n",
    "\n",
    "# The trainning data must be in a zip folder.\n",
    "train_zip_file_name  = [file_name for file_name in os.listdir(train_data_path) if file_name.endswith('.zip')][0]\n",
    "train_zip_file_path = os.path.join(train_data_path,train_zip_file_name)\n",
    "train_folder_path = train_zip_file_path.replace('.zip','')\n",
    "\n",
    "print('train_zip_file_path',train_zip_file_path)\n",
    "print('train_folder_path',train_folder_path)\n",
    "print('tipo_train_path',type(train_folder_path))\n",
    "\n",
    "zip_file = zipfile.ZipFile(train_zip_file_path)\n",
    "zip_file.extractall(train_data_path)\n",
    "zip_file.close()\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(train_folder_path) if f.endswith('.shp')]\n",
    "classes = [f.split('.')[0] for f in files]\n",
    "\n",
    "shapefiles = [os.path.join(train_folder_path, f) for f in files if f.endswith('.shp')]\n",
    "print('tipo shapefiles')\n",
    "print(type(shapefiles))\n",
    "print(shapefiles)\n",
    "\n",
    "\n",
    "rows, cols = xarr0[product['bands'][0]].shape\n",
    "\n",
    "print('rows',rows)\n",
    "print('cols',cols)\n",
    "\n",
    "_coords=xarr0.coords\n",
    "\n",
    "print('bandas xarr0',list(xarr0.data_vars))\n",
    "lista=list(xarr0.data_vars)\n",
    "\n",
    "#(originX, pixelWidth, 0, originY, 0, pixelHeight)\n",
    "geo_transform=(_coords[\"longitude\"].values[0], 0.000269995,0, _coords[\"latitude\"].values[0],0,-0.000271302)\n",
    "proj = xarr0.crs.crs_wkt\n",
    "\n",
    "#print('shapefile_docs',shapefiles)\n",
    "\n",
    "labeled_pixels = rasterizar_entrenamiento(shapefiles, rows, cols, geo_transform, proj)\n",
    "\n",
    "print('data source')\n",
    "\n",
    "is_train = np.nonzero(labeled_pixels)\n",
    "training_labels = labeled_pixels[is_train]\n",
    "\n",
    "# Preprocesar:\n",
    "#nmed=None\n",
    "#nan_mask=None\n",
    "#xarrs=list(xarr0.values())\n",
    "#print(type(xarrs))\n",
    "#medians1 = xarrs[0]\n",
    "\n",
    "#print(\"medianas\")\n",
    "#print(type(medians1))\n",
    "print(\"medianas\",xarr0)\n",
    "print(\"fin consulta mediana\")\n",
    "#print('medians1.datavars',medians1.data_vars.keys())\n",
    "\n",
    "\n",
    "bands_data=[]\n",
    "\n",
    "\n",
    "#bands2=list(xarr0.data_vars.keys())\n",
    "\n",
    "\n",
    "for band in lista:\n",
    "    #print('bands',product['bands'])\n",
    "    # pixel_qa is removed from xarr0 by Compuesto Temporal de Medianas\n",
    "    if band != 'pixel_qa':\n",
    "        bands_data.append(xarr0[band])\n",
    "bands_data = np.dstack(bands_data)\n",
    "training_samples = bands_data[is_train]\n",
    "print('training_samples')\n",
    "print(training_samples.shape)\n",
    "\n",
    "rows, cols, n_bands = bands_data.shape\n",
    "\n",
    "np.isfinite(training_samples)\n",
    "_msk=np.sum(np.isfinite(training_samples),1)>1\n",
    "training_samples= training_samples[_msk,:]\n",
    "training_labels=training_labels[_msk]\n",
    "\n",
    "#mascara valores nan por valor no data\n",
    "mask_nan=np.isnan(training_samples)\n",
    "training_samples[mask_nan]=-9999\n",
    "print('training_samples')\n",
    "print(training_samples)\n",
    "\n",
    "print('training_labels')\n",
    "print(training_labels)\n",
    "\n",
    "\n",
    "print('training_labels')\n",
    "print(training_labels.shape)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#%%time\n",
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators=500, verbose=1)\n",
    "dtree=tree.DecisionTreeClassifier(criterion='gini')\n",
    "svml=SVC(C=1.0,  class_weight='balanced',decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "           max_iter=-1, probability=False, random_state=None, shrinking=True,tol=0.001, verbose=False)\n",
    "#knn = KNeighborsClassifier(algorithm='brute',n_neighbors=3,metric='mahalanobis')\n",
    "nn = MLPClassifier(alpha=0.0001,  hidden_layer_sizes=(500,),random_state=None,max_iter=500,activation = 'logistic',solver='adam')\n",
    "grad_boost=GradientBoostingClassifier(n_estimators=500,learning_rate=1)\n",
    "extrat = ExtraTreesClassifier(n_estimators=50, max_depth=None,class_weight='balanced')\n",
    "\n",
    "clf_array=[rf]#extrat,dtree,nn,svml,grad_boost,rf]#svml,nn,grad_boost,extrat,dtree\n",
    "\n",
    "for clf in clf_array:\n",
    "    vanilla_scores = cross_val_score(clf, training_samples, training_labels, cv=2, n_jobs=-1)\n",
    "    bagging_clf = BaggingClassifier(clf)\n",
    "    bagging_scores = cross_val_score(bagging_clf,training_samples, training_labels, cv=2, \n",
    "       n_jobs=-1)\n",
    "\n",
    "    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f}[{0}]\"  \n",
    "                       .format(clf.__class__.__name__, \n",
    "                       vanilla_scores.mean(), vanilla_scores.std()))\n",
    "    print (\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\\n\"\n",
    "                       .format(clf.__class__.__name__, \n",
    "                        bagging_scores.mean(), bagging_scores.std()))\n",
    "\n",
    "\n",
    "\n",
    "# Split train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_samples, training_labels, test_size=0.3)\n",
    "\n",
    "print(f'Train X {len(X_train)/len(training_samples)*100:.2f}%')\n",
    "print(f'Train Y {len(y_train)/len(training_samples)*100:.2f}%')\n",
    "print(f'Test  X {len(X_test )/len(training_samples)*100:.2f}%')\n",
    "print(f'Test  Y {len(y_test )/len(training_samples)*100:.2f}%')\n",
    "\n",
    "print('trainning samples',X_train)\n",
    "print('trainning labels',y_train)\n",
    "\n",
    "\n",
    "\n",
    "#bagging_clf.fit(training_samples, training_labels)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculo de y_pred\n",
    "print('Estimar y con datos de entrada')\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Calculo de matrix de confusion\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, precision_score\n",
    "\n",
    "mconf = confusion_matrix(y_test,y_pred)\n",
    "# Calculo de kappa score\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "# Calculo de precision score\n",
    "prec = precision_score(y_test, y_pred,average = 'weighted')\n",
    "\n",
    "# Save metrics to file\n",
    "with open(posixpath.join(folder+'metrics.txt'),'w') as file_metrics:\n",
    "    print(f'matriz de confusion: {mconf}')\n",
    "    print(f'kappa score: {kappa}')\n",
    "    print(f'precision score (weighted): {prec}')\n",
    "    file_metrics.write('matriz de confusion: \\n'+str(mconf))\n",
    "    file_metrics.write('\\nkappa score: '+str(kappa))\n",
    "    file_metrics.write('\\nprecision score (weighted): '+str(prec))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('bagging mean')\n",
    "print(bagging_scores.mean())\n",
    "\n",
    "print('bagging scores')\n",
    "print(bagging_scores.mean())\n",
    "\n",
    "print('bagging clf')\n",
    "print(bagging_clf)\n",
    "\n",
    "\n",
    "# write shapefiles list\n",
    "file = open(folder+\"shapefiles_list.txt\", \"w\")\n",
    "file.write(\"shapefiles list = \" + \"\\n\".join(shapefiles))\n",
    "file.close()\n",
    "\n",
    "\n",
    "outputxcom=posixpath.join(folder,'modelo_random_forest_2.pkl')\n",
    "with open(outputxcom, 'wb') as fid:\n",
    "    joblib.dump(bagging_clf, fid)\n",
    "\n",
    "\n",
    "#classified = outputxcom.classified\n",
    "#classified.values = classified.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "# Preprocesar:\n",
    "nmed=None\n",
    "nan_mask=None\n",
    "xarrs=list(xarrs.values())\n",
    "print(type(xarrs))\n",
    "medians1 = xarrs[0]\n",
    "print(type(medians1))\n",
    "print(\"medianas\")\n",
    "bands_data = []\n",
    "\n",
    "print('medians1.datavars',medians1.data_vars.keys())\n",
    "\n",
    "#rows, cols = xarr0[product['bands'][0]].shape\n",
    "\n",
    "\n",
    "bands_data=[]\n",
    "\n",
    "bands2=list(medians1.data_vars.keys())\n",
    "print('bands2',bands2)\n",
    "\n",
    "\n",
    "for band in bands2: \n",
    "\tif band != 'pixel_qa':\n",
    "   \t bands_data.append(medians1[band])\n",
    "bands_data = np.dstack(bands_data)\n",
    "\n",
    "print('bands_data_test2',bands_data)\n",
    "\n",
    "\n",
    "rows, cols, n_bands = bands_data.shape\n",
    "\n",
    "print('rows',rows)\n",
    "print('cols',cols)\n",
    "\n",
    "print('n_bands',n_bands)\n",
    "\n",
    "\n",
    "\n",
    "n_samples = rows*cols\n",
    "flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "#mascara valores nan por valor no data\n",
    "mask_nan=np.isnan(flat_pixels)\n",
    "flat_pixels[mask_nan]=-9999\n",
    "#result = bagging_clf.predict(flat_pixels)\n",
    "#classification = result.reshape((rows, cols))\n",
    "import os\n",
    "\n",
    "print(\"modelo\")\n",
    "\n",
    "model = None\n",
    "for file in other_files:\n",
    "    if file.endswith(\".pkl\"):\n",
    "        model = file\n",
    "        break\n",
    "if model is None:\n",
    "    raise \"Debería haber un modelo en la carpeta \" + modelos\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    bagging_clf = joblib.load(model)\n",
    "    print(bagging_clf)\n",
    "\n",
    "#print(bagging_clf)\n",
    "n_samples = rows*cols\n",
    "flat_pixels = bands_data.reshape((n_samples, n_bands))\n",
    "#mascara valores nan por valor no data\n",
    "mask_nan=np.isnan(flat_pixels)\n",
    "flat_pixels[mask_nan]=-9999\n",
    "\n",
    "print('flat_pixels')\n",
    "print(type(flat_pixels))\n",
    "print(flat_pixels.min())\n",
    "print(flat_pixels.max())\n",
    "print(flat_pixels)\n",
    "\n",
    "\n",
    "from numpy import inf\n",
    "                        \n",
    "flat_pixels[flat_pixels<0]=0\n",
    "flat_pixels[flat_pixels==-inf]= 0\n",
    "flat_pixels[flat_pixels>= 1E308]=0\n",
    "                                    \n",
    "\n",
    "print('flat_pixels 3')\n",
    "print(type(flat_pixels))\n",
    "print(flat_pixels.min())\n",
    "print(flat_pixels.max())\n",
    "print(flat_pixels)                          \n",
    "#np.isfinite(flat_pixels)\n",
    "\n",
    "_msk=np.sum(np.isfinite(flat_pixels),1)>1\n",
    "#flat_pixels= flat_pixels[_msk,:]\n",
    "flat_pixels=flat_pixels[_msk]\n",
    "\n",
    "\n",
    "print(flat_pixels.min())\n",
    "print(flat_pixels.max())                      \n",
    "#result = bagging_clf.predict(flat_pixels)\n",
    "#classification = result.reshape((rows, cols))\n",
    "\n",
    "print(\"clasificacion final\")\n",
    "result = bagging_clf.predict(flat_pixels)\n",
    "result = result.reshape((rows, cols))\n",
    "print(\"fin funcion de clasificacion\")\n",
    "print(result)\n",
    "\n",
    "print('tipo result')\n",
    "print(type(result))\n",
    "\n",
    "#result = bagging_clf.predict(nmed.T)\n",
    "#result = result.reshape(sp)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "coordenadas = []\n",
    "dimensiones = []\n",
    "xcords = {}\n",
    "for coordenada in xarrs[0].coords:\n",
    "    if (coordenada != 'time'):\n",
    "        coordenadas.append((coordenada, xarrs[0].coords[coordenada]))\n",
    "        dimensiones.append(coordenada)\n",
    "        xcords[coordenada] = xarrs[0].coords[coordenada]\n",
    "\n",
    "valores = {\"classified\": xr.DataArray(result, dims=dimensiones, coords=coordenadas)}\n",
    "#array = xr.DataArray(result, dims=dimensiones, coords=coordenadas)\n",
    "#array.astype('float32')\n",
    "#valores = {\"classified\": array}\n",
    "\n",
    "output = xr.Dataset(valores, attrs={'crs': xarrs[0].crs})\n",
    "for coordenada in output.coords:\n",
    "    output.coords[coordenada].attrs[\"units\"] = xarrs[0].coords[coordenada].units\n",
    "\n",
    "classified = output.classified\n",
    "classified.values = classified.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from arrnorm.auxil.auxil import similarity\n",
    "#import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from arrnorm.auxil.auxil import orthoregress\n",
    "from operator import itemgetter\n",
    "from scipy import linalg, stats\n",
    "import arrnorm.auxil.auxil as auxil\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "# Preprocesar:\n",
    "nmed=None\n",
    "nan_mask=None\n",
    "print('values del directorio')\n",
    "xarr0=list(xarrs.values())\n",
    "print(type(xarrs))\n",
    "\n",
    "#mosaico de LS8\n",
    "\n",
    "print(xarrs.keys())\n",
    "inDataset1=[xarrs[k] for k in xarrs.keys() if 'clasificador' in k][0];\n",
    "#inDataset1.sortby('latitude', ascending=False)\n",
    "inDataset1 = inDataset1.sortby('latitude', ascending=False)\n",
    "\n",
    "print('plot_objetivo_inDataset1')\n",
    "print(inDataset1)\n",
    "\n",
    "#consulta de mosaico\n",
    "inDataset2=[xarrs[k] for k in xarrs.keys() if 'consulta_referencia' in k][0];\n",
    "print('plot_referencia_inDataset2')\n",
    "print(inDataset2)\n",
    "print(type(inDataset2))\n",
    "\n",
    "print(inDataset2[\"fnf_mask\"].values[0]==1)\n",
    "fnf_mas=np.isin(inDataset2[\"fnf_mask\"].values[0]==1, np.nan)\n",
    "print('la mascara es tipo')\n",
    "print(type(fnf_mas))\n",
    "print(fnf_mas)\n",
    "\n",
    "fnf_mask_tmp=xr.DataArray(inDataset2[\"fnf_mask\"])\n",
    "\n",
    "fnf_mask=np.isin(fnf_mask_tmp.values[0]==1, np.nan)\n",
    "print('la mascara temporal 2 es tipo')\n",
    "print(type(fnf_mask_tmp))\n",
    "print(fnf_mask)\n",
    "\n",
    "print('plot_tipo_objetivo_inDataset1')\n",
    "print(type(inDataset1))\n",
    "\n",
    "inDataset12=xr.DataArray(inDataset1[\"classified\"])\n",
    "\n",
    "\n",
    "#ImgResultadof=np.where(fnf_mask_tmp.values[0]==1, np.nan,inDataset12)\n",
    "# replace np.nan by 0 to set forest mask to 0\n",
    "print(\"fnf masked to 0\")\n",
    "ImgResultadof=np.where(fnf_mask_tmp.values[0]==1, 0,inDataset12)\n",
    "\n",
    "print(ImgResultadof)\n",
    "output=ImgResultadof\n",
    "\n",
    "print(\"termino funcion\")\n",
    "#print(type(inDataset1[0]))\n",
    "print(\"asignacion de dataset\")\n",
    "xarrs[0]=inDataset1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ncoords=[]\n",
    "xdims =[]\n",
    "xcords={}\n",
    "for x in xarrs[0].coords:\n",
    "    if(x!='time'):\n",
    "        ncoords.append( ( x, xarrs[0].coords[x]) )\n",
    "        xdims.append(x)\n",
    "        xcords[x]=xarrs[0].coords[x]\n",
    "variables ={\"mask\": xr.DataArray(output, dims=xdims,coords=ncoords)}\n",
    "output=xr.Dataset(variables, attrs={'crs':xarrs[0].crs})\n",
    "for x in output.coords:\n",
    "    output.coords[x].attrs[\"units\"]=xarrs[0].coords[x].units\n",
    "\n",
    "print(\"clasificacion_mask\")\n",
    "#print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
